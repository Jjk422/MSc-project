{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"ITIL Books/ITIL 3/ITIL3 Service Operation chapter 4.pdf\"\n",
    "# extracted_text_file_path = \"ITIL Books/ITIL 3/Continual service improvement chapter from notebook.txt\"\n",
    "# extracted_text_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Service operation chapter 4 - 4.txt\"\n",
    "# extracted_text_file_path = \"ITIL Books\\ITIL 3\\Service operation chapter 4\\Automated concepts extracted\\\\4.2\\Service operation chapter 4 - 4.2 to 4.2.4 .txt\"\n",
    "output_file_path = \"output/ITIL3 Continual Service Improvement.txt\"\n",
    "\n",
    "### Chapter 4 start ###\n",
    "# extracted_text_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Service operation chapter 4 - 4.txt\"\n",
    "# manual_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4/Manual concepts extracted 4.txt\"\n",
    "\n",
    "### Chapter 4 - 4.1 to 4.1.4 ###\n",
    "extracted_text_file_path = \"ITIL Books\\ITIL 3\\Service operation chapter 4\\Automated concepts extracted\\\\4.1\\Service operation chapter 4 - 4.1 to 4.1.4.txt\"\n",
    "manual_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4.1/Manual concepts extracted 4.1.txt\"\n",
    "\n",
    "### Chapter 4 - 4.2 to 4.2.4 ###\n",
    "# extracted_text_file_path = \"ITIL Books\\ITIL 3\\Service operation chapter 4\\Automated concepts extracted\\\\4.2\\Service operation chapter 4 - 4.2 to 4.2.4.txt\"\n",
    "# manual_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4.2/Manual concepts extracted 4.2.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open(extracted_text_file_path, 'r') as file:\n",
    "    extracted_text = file.read()\n",
    "    \n",
    "# extracted_text = \"Particle dynamics involves the study of physics and chemistry\"\n",
    "\n",
    "# tokens = nltk.word_tokenize(extracted_text)\n",
    "# print(tokens)\n",
    "\n",
    "# ### Part of speech tagging ###\n",
    "# part_of_speech_array = nltk.pos_tag(tokens)\n",
    "# print(part_of_speech_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    },
    "scrolled": false
   },
   "source": [
    "Text sanitization and word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Grab sections from text ###\n",
    "# print(re.findall(\"^\\d(\\.|\\d)*(\\s|\\w)*$\", extracted_text))\n",
    "# title_pattern = re.compile(r\"^\\d(\\.|\\d)*(\\s|\\w)*$\", re.MULTILINE)\n",
    "title_pattern = re.compile(r\"^\\d+.*$\", re.MULTILINE)\n",
    "\n",
    "sections = title_pattern.findall(extracted_text)\n",
    "for counter, section in enumerate(sections):\n",
    "    if not (section.find(\"%\") == -1 and section.find(\")\") == -1):\n",
    "        sections.remove(section)\n",
    "\n",
    "### Sanitise extracted text ###\n",
    "extracted_text_sanitised = extracted_text\n",
    "extracted_text_sanitised = extracted_text.replace(\"¦\", \"\")\n",
    "extracted_text_sanitised = extracted_text_sanitised.replace(\"–\", \"\")\n",
    "        \n",
    "### Tokenise extracted text ###\n",
    "tokens = nltk.word_tokenize(extracted_text_sanitised)\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part to speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Part of speech tagging ###\n",
    "part_of_speech_array = nltk.pos_tag(tokens)\n",
    "# print(part_of_speech_array)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "part_of_speech_array_lemmatized = []\n",
    "\n",
    "for part_of_speech in part_of_speech_array:\n",
    "    part_of_speech_array_lemmatized.append(\n",
    "        (lemmatizer.lemmatize(part_of_speech[0]), part_of_speech[1])\n",
    "    ) \n",
    "    \n",
    "# print(part_of_speech_array_lemmatized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Term extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "major named concepts: \n['INCIDENT MANAGEMENT', 'ITIL', 'IT', 'IT', 'Management', 'Service Desk', 'Purpose/goal/objective', 'Incident Management', '‘ Normal', '’', 'SLA', 'Scope Incident Management', 'Service Desk', 'Event Management', 'Incident Management', 'Service Desk', 'Service Desk', 'Service', '’', 'SLA', 'Service', 'Request Fulfilment', 'Value', 'Incident Management', 'IT', 'Incident Management', 'Service Desk', 'IT', 'Incident Management', 'Service Operation', 'Incident Management', 'Service Management', 'Management']\n\nother concepts: \n['terminology', 'incident ’', 'interruption', 'service', 'reduction', 'quality', 'service', 'Failure', 'configuration item', 'service', 'incident', 'example failure', 'disk', 'mirror set', 'process', 'incidents', 'failures', 'questions', 'queries', 'users', 'telephone call', 'staff', 'event monitoring tools', 'goal', 'process', 'service operation', 'impact', 'business operations', 'levels', 'service quality', 'availability', 'service operation', 'service operation', 'limits', 'event', 'service', 'events', 'users', 'interface', 'tools', 'Incidents', 'staff', 'example', 'something', 'hardware', 'network component', 'incident', 'events', 'incidents', 'classes', 'events', 'disruptions', 'indicators', 'operation', 'section', 'incidents', 'service requests', 'requests', 'disruption', 'service', 'way', 'customer', 's needs', 'target', 'requests', 'process', 'section', 'business', 'value', 'ability', 'incidents', 'results', 'downtime', 'business', 'turn', 'availability', 'service', 'business', 'functionality', 'service', 'ability', 'activity', 'real-time business priorities', 'capability', 'business priorities', 'resources', 'ability', 'improvements', 'services', 'result', 'incident', 'contact', 'activities', 'business', 'staff', 'handling', 'incidents', 'service', 'training requirements', 'business', 'business', 'value', 'areas', 'reason', 'projects', 'benefit', 'areas', 'attention', 'justification', 'expenditure', 'processes']\n\nall noun phrases: \n['INCIDENT MANAGEMENT', 'ITIL terminology', 'incident ’', 'interruption', 'IT service', 'reduction', 'quality', 'IT service', 'Failure', 'configuration item', 'service', 'incident', 'example failure', 'disk', 'mirror set', 'Management', 'process', 'incidents', 'failures', 'questions', 'queries', 'users', 'telephone call', 'Service Desk', 'staff', 'event monitoring tools', 'Purpose/goal/objective', 'goal', 'Incident Management process', 'service operation', 'impact', 'business operations', 'levels', 'service quality', 'availability', '‘ Normal service operation ’', 'service operation', 'SLA limits', 'Scope Incident Management', 'event', 'service', 'events', 'users', 'Service Desk', 'interface', 'Event Management', 'Incident Management tools', 'Incidents', 'staff', 'example', 'something', 'hardware', 'network component', 'incident', 'Service Desk', 'events', 'incidents', 'classes', 'events', 'disruptions', 'indicators', 'operation', 'section', 'incidents', 'service requests', 'Service Desk', 'Service requests', 'disruption', 'service', 'way', 'customer ’ s needs', 'target', 'SLA', 'Service requests', 'Request Fulfilment process', 'section', 'Value', 'business', 'value', 'Incident Management', 'ability', 'incidents', 'results', 'downtime', 'business', 'turn', 'availability', 'service', 'business', 'functionality', 'service', 'ability', 'IT activity', 'real-time business priorities', 'Incident Management', 'capability', 'business priorities', 'resources', 'ability', 'improvements', 'services', 'result', 'incident', 'contact', 'activities', 'business', 'staff', 'Service Desk', 'handling', 'incidents', 'service', 'training requirements', 'IT', 'business', 'Incident Management', 'business', 'value', 'areas', 'Service Operation', 'reason', 'Incident Management', 'Service Management projects', 'benefit', 'Management', 'areas', 'attention', 'justification', 'expenditure', 'processes']\n\nall noun phrases with adj: \n['INCIDENT MANAGEMENT', 'ITIL terminology', '‘ incident ’', 'unplanned interruption', 'IT service', 'reduction', 'quality', 'IT service', 'Failure', 'configuration item', 'service', 'incident', 'example failure', 'disk', 'mirror set', 'Incident Management', 'process', 'incidents', 'failures', 'questions', 'queries', 'users', 'telephone call', 'Service Desk', 'technical staff', 'event monitoring tools', 'Purpose/goal/objective', 'primary goal', 'Incident Management process', 'normal service operation', 'possible', 'adverse impact', 'business operations', 'possible levels', 'service quality', 'availability', '‘ Normal service operation ’', 'service operation', 'SLA limits', 'Scope Incident Management', 'event', 'service', 'events', 'users', 'Service Desk', 'interface', 'Event Management', 'Incident Management tools', 'Incidents', 'technical staff', 'example', 'something untoward', 'hardware', 'network component', 'incident', 'Service Desk', 'events', 'incidents', 'Many classes', 'events', 'disruptions', 'indicators', 'normal operation', 'informational', 'section', 'incidents', 'service requests', 'Service Desk', 'same', 'Service requests', 'disruption', 'service', 'way', 'customer ’ s needs', 'agreed target', 'SLA', 'Service requests', 'Request Fulfilment process', 'section', 'Value', 'business', 'value', 'Incident Management', 'ability', 'incidents', 'results', 'downtime', 'business', 'turn', 'availability', 'service', 'business', 'able', 'functionality', 'service', 'ability', 'IT activity', 'real-time business priorities', 'Incident Management', 'capability', 'business priorities', 'allocate resources', 'necessary', 'ability', 'potential improvements', 'services', 'result', 'incident', 'contact', 'activities', 'business operational staff', 'Service Desk', 'handling', 'incidents', 'additional service', 'training requirements', 'IT', 'business', 'Incident Management', 'visible', 'business', 'value', 'areas', 'Service Operation', 'reason', 'Incident Management', 'first', 'Service Management projects', 'added benefit', 'Incident Management', 'other areas', 'attention', 'justification', 'expenditure', 'other processes']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "### Term Extraction (NNP next to each other) ###\n",
    "def extract_terms(part_of_speech_array_array, tags_to_use):\n",
    "    terms_array = []\n",
    "    term_phrase = []\n",
    "    start_new_term = True\n",
    "    for index, part in enumerate(part_of_speech_array_array):\n",
    "        if(part[1] in tags_to_use):\n",
    "            term_phrase.append(part[0])\n",
    "            start_new_term = False if part_of_speech_array_array[index + 1][1] in tags_to_use else True\n",
    "\n",
    "            if start_new_term == True:\n",
    "                terms_array.append(\" \".join(term_phrase))\n",
    "                term_phrase = []\n",
    "    return terms_array\n",
    "\n",
    "### Term Extraction (NNP next to each other) ###\n",
    "def extract_terms_with_adj(part_of_speech_array_array, tags_to_use):\n",
    "    terms_array = []\n",
    "    term_phrase = []\n",
    "    start_new_term = True\n",
    "    for index, part in enumerate(part_of_speech_array_array):\n",
    "        if(part[1] in tags_to_use):\n",
    "            term_phrase.append(part[0])\n",
    "            start_new_term = False if part_of_speech_array_array[index + 1][1] in tags_to_use else True\n",
    "\n",
    "            if start_new_term == True:\n",
    "                terms_array.append(\" \".join(term_phrase))\n",
    "                term_phrase = []\n",
    "    return terms_array\n",
    "\n",
    "major_named_concepts = extract_terms(part_of_speech_array, {\"NNP\", \"NNPS\"})\n",
    "other_concepts = extract_terms(part_of_speech_array, {\"NN\", \"NNS\"})\n",
    "all_noun_phrases = extract_terms(part_of_speech_array, {\"NNP\", \"NNPS\", \"NN\", \"NNS\"})\n",
    "all_noun_phrases_with_adj = extract_terms_with_adj(part_of_speech_array, {\"NNP\", \"NNPS\", \"NN\", \"NNS\", \"JJ\"})\n",
    "\n",
    "print(f\"major named concepts: \\n{major_named_concepts}\")\n",
    "print(f\"\\nother concepts: \\n{other_concepts}\")\n",
    "print(f\"\\nall noun phrases: \\n{all_noun_phrases}\")\n",
    "print(f\"\\nall noun phrases with adj: \\n{all_noun_phrases_with_adj}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Major/common concept extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "major named concepts: \n[('Incident Management', 6), ('Service Desk', 5), ('IT', 4), ('Management', 2), ('’', 2), ('SLA', 2), ('Service', 2), ('INCIDENT MANAGEMENT', 1), ('ITIL', 1), ('Purpose/goal/objective', 1), ('‘ Normal', 1), ('Scope Incident Management', 1), ('Event Management', 1), ('Request Fulfilment', 1), ('Value', 1), ('Service Operation', 1), ('Service Management', 1)]\n\nother concepts: \n[('service', 8), ('business', 6), ('incidents', 5), ('incident', 3), ('process', 3), ('staff', 3), ('service operation', 3), ('events', 3), ('ability', 3), ('users', 2), ('availability', 2), ('section', 2), ('requests', 2), ('value', 2), ('areas', 2), ('terminology', 1), ('incident ’', 1), ('interruption', 1), ('reduction', 1), ('quality', 1), ('Failure', 1), ('configuration item', 1), ('example failure', 1), ('disk', 1), ('mirror set', 1), ('failures', 1), ('questions', 1), ('queries', 1), ('telephone call', 1), ('event monitoring tools', 1), ('goal', 1), ('impact', 1), ('business operations', 1), ('levels', 1), ('service quality', 1), ('limits', 1), ('event', 1), ('interface', 1), ('tools', 1), ('Incidents', 1), ('example', 1), ('something', 1), ('hardware', 1), ('network component', 1), ('classes', 1), ('disruptions', 1), ('indicators', 1), ('operation', 1), ('service requests', 1), ('disruption', 1)]\n\nall noun phrases: \n[('service', 6), ('business', 6), ('incidents', 5), ('Service Desk', 5), ('Incident Management', 4), ('incident', 3), ('staff', 3), ('events', 3), ('ability', 3), ('IT service', 2), ('Management', 2), ('users', 2), ('service operation', 2), ('availability', 2), ('section', 2), ('Service requests', 2), ('value', 2), ('areas', 2), ('INCIDENT MANAGEMENT', 1), ('ITIL terminology', 1), ('incident ’', 1), ('interruption', 1), ('reduction', 1), ('quality', 1), ('Failure', 1), ('configuration item', 1), ('example failure', 1), ('disk', 1), ('mirror set', 1), ('process', 1), ('failures', 1), ('questions', 1), ('queries', 1), ('telephone call', 1), ('event monitoring tools', 1), ('Purpose/goal/objective', 1), ('goal', 1), ('Incident Management process', 1), ('impact', 1), ('business operations', 1), ('levels', 1), ('service quality', 1), ('‘ Normal service operation ’', 1), ('SLA limits', 1), ('Scope Incident Management', 1), ('event', 1), ('interface', 1), ('Event Management', 1), ('Incident Management tools', 1), ('Incidents', 1)]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# concept_relationships = extract_terms(part_of_speech_array, {\"VP\"})\n",
    "# print(concept_relationships)\n",
    "# print(all_noun_phrases)\n",
    "\n",
    "### Perform frequency analysis ###\n",
    "### Concept Extraction Frequency analysis ###\n",
    "major_named_concept_frequency_distribution = nltk.FreqDist(major_named_concepts)\n",
    "other_concept_frequency_distribution = nltk.FreqDist(other_concepts)\n",
    "all_noun_phrases_frequency_distribution = nltk.FreqDist(all_noun_phrases)\n",
    "\n",
    "print(f\"major named concepts: \\n{major_named_concept_frequency_distribution.most_common(50)}\")\n",
    "print(f\"\\nother concepts: \\n{other_concept_frequency_distribution.most_common(50)}\")\n",
    "print(f\"\\nall noun phrases: \\n{all_noun_phrases_frequency_distribution.most_common(50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Concept relationship extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_at_index(part_of_speech_array, index):\n",
    "    sentence_starting_index = 0\n",
    "    sentence_end_index = len(part_of_speech_array)\n",
    "    \n",
    "    ### Get sentence start index ###\n",
    "    for i in range(0, index):\n",
    "#         print(part_of_speech_array[index])\n",
    "        if part_of_speech_array[index - i][1] == \".\":\n",
    "            sentence_starting_index = index - i\n",
    "            break\n",
    "    \n",
    "    ### Get sentence end index ###\n",
    "    for i in range(0, len(part_of_speech_array)):\n",
    "        if part_of_speech_array[index + i][1] == \".\":\n",
    "            sentence_end_index = index + i\n",
    "            break\n",
    "            \n",
    "    return (sentence_starting_index, sentence_end_index + 1)\n",
    "\n",
    "### Term Extraction (NNP next to each other) ###\n",
    "def extract_terms(part_of_speech_array_array, tags_to_use):\n",
    "    part_of_speech_array_with_terms = []\n",
    "    \n",
    "    terms_array = []\n",
    "    term_phrase = []\n",
    "    start_new_term = True\n",
    "    for index, part in enumerate(part_of_speech_array_array):\n",
    "        if(part[1] in tags_to_use):\n",
    "            term_phrase.append(part[0])\n",
    "            start_new_term = False if part_of_speech_array_array[index + 1][1] in tags_to_use else True\n",
    "\n",
    "            if start_new_term == True:\n",
    "                if len(term_phrase) > 1:\n",
    "#                     part_of_speech_array_with_terms.append((\" \".join(term_phrase), f\"NPhrase-{part[1]}\"))\n",
    "                    part_of_speech_array_with_terms.append((\" \".join(term_phrase), \"NPhrase\"))\n",
    "                else:\n",
    "                    part_of_speech_array_with_terms.append((\" \".join(term_phrase), part[1]))\n",
    "                term_phrase = []\n",
    "        else:\n",
    "            part_of_speech_array_with_terms.append((part[0], part[1]))\n",
    "    return part_of_speech_array_with_terms\n",
    "\n",
    "part_of_speech_array_with_terms = extract_terms(part_of_speech_array, {\"NNP\", \"NNPS\", \"NN\", \"NNS\"})\n",
    "# print(part_of_speech_array_with_terms)\n",
    "\n",
    "sentences = []\n",
    "temp_sentence = []\n",
    "for word in part_of_speech_array_with_terms:\n",
    "    if word[1] is \".\":\n",
    "        temp_sentence.append(word)\n",
    "        sentences.append(temp_sentence)\n",
    "        temp_sentence = []\n",
    "    else:\n",
    "        temp_sentence.append(word)\n",
    "        \n",
    "        \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# for index, sentence in enumerate(sentences):\n",
    "#     sentence_with_no_stop_words = [word_pos for word_pos in sentence if not word_pos[0] in stop_words]\n",
    "#     \n",
    "#     if index > 3:\n",
    "#         break\n",
    "#         \n",
    "#     print(sentence)\n",
    "#     print(sentence_with_no_stop_words)\n",
    "#     print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[('4.2.1', 'CD'), ('Purpose/goal/objective', 'NNPS'), ('The', 'DT'), ('primary', 'JJ'), ('goal', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Incident Management process', 'NPhrase'), ('is', 'VBZ'), ('to', 'TO'), ('restore', 'VB'), ('normal', 'JJ'), ('service operation', 'NPhrase'), ('as', 'RB'), ('quickly', 'RB'), ('as', 'IN'), ('possible', 'JJ'), ('and', 'CC'), ('minimize', 'VB'), ('the', 'DT'), ('adverse', 'JJ'), ('impact', 'NN'), ('on', 'IN'), ('business operations', 'NPhrase'), (',', ','), ('thus', 'RB'), ('ensuring', 'VBG'), ('that', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('possible', 'JJ'), ('levels', 'NNS'), ('of', 'IN'), ('service quality', 'NPhrase'), ('and', 'CC'), ('availability', 'NN'), ('are', 'VBP'), ('maintained', 'VBN'), ('.', '.')]\n*<JJ><NNP><VBZ><NP>\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# from part_of_speech_regex import PartOfSpeechRegex\n",
    "\n",
    "class PartOfSpeechRegex:\n",
    "    def parseAndReturnPatterns(self, pattern_string, sentence):\n",
    "        print(pattern_string)\n",
    "\n",
    "pattern_string = \"*<JJ><NNP><VBZ><NP>\"\n",
    "\n",
    "print(sentences[3])\n",
    "\n",
    "part_of_speech_regex = PartOfSpeechRegex()\n",
    "# part_of_speech_regex.parseAndReturnPatterns()\n",
    "\n",
    "part_of_speech_regex.parseAndReturnPatterns(pattern_string, sentences[3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%        \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "most_common_major_concepts = major_named_concept_frequency_distribution.most_common(50)\n",
    "# print(most_common_major_concepts)\n",
    "# print(tokens.index(most_common_major_concepts[0][0]))\n",
    "# print(part_of_speech_array[206])\n",
    "## Get indices of all common concepts\n",
    "indices = [i for i, x in enumerate(part_of_speech_array) if x[0] == most_common_major_concepts[0][0]]\n",
    "# print(indices)\n",
    "# print(most_common_major_concepts[1][0])\n",
    "\n",
    "def get_sentence_at_index(part_of_speech_array, index):\n",
    "    sentence_starting_index = 0\n",
    "    sentence_end_index = len(part_of_speech_array)\n",
    "    \n",
    "    ### Get sentence start index ###\n",
    "    for i in range(0, index):\n",
    "        if part_of_speech_array[index - i][1] == \".\":\n",
    "            sentence_starting_index = index - i\n",
    "            break\n",
    "    \n",
    "    ### Get sentence end index ###\n",
    "    for i in range(0, index):\n",
    "        if part_of_speech_array[index + i][1] == \".\":\n",
    "            sentence_end_index = index + i\n",
    "            break\n",
    "            \n",
    "    return (sentence_starting_index + 1, sentence_end_index + 1)\n",
    "\n",
    "def does_list_contain_verb_pos(part_of_speech_array):\n",
    "    for word_pos in part_of_speech_array:\n",
    "        if word_pos[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "# sentence_index = get_sentence_at_index(part_of_speech_array, indices[0])\n",
    "# print(sentence_index)\n",
    "\n",
    "concept_relations = []\n",
    "\n",
    "i = 0\n",
    "for index in indices:\n",
    "#     if i < 3:\n",
    "#         i = i + 1\n",
    "#         continue\n",
    "    sentence_pos_containing_concept = part_of_speech_array[\n",
    "        get_sentence_at_index(part_of_speech_array, index)[0]:\n",
    "        get_sentence_at_index(part_of_speech_array, index)[1]\n",
    "    ]\n",
    "    print(sentence_pos_containing_concept)\n",
    "        \n",
    "    last_concept = ()\n",
    "    last_concept_index = -1\n",
    "    # For word part_of_speech in sentence_part_of_speech_containing_concept\n",
    "    for index, word_pos in enumerate(sentence_pos_containing_concept):\n",
    "#         print(f\"{word_pos[0]}: {word_pos[1]}\")\n",
    "#         print(word_pos[0] in all_noun_phrases)\n",
    "        \n",
    "#         if (word_pos[0] in all_noun_phrases):\n",
    "        if (word_pos[0] in major_named_concepts):\n",
    "            if last_concept_index != -1:# and does_list_contain_verb_pos(sentence_pos_containing_concept[last_concept_index + 1:index]):\n",
    "                concept_relations.append(f\"{last_concept}::{sentence_pos_containing_concept[last_concept_index + 1:index]}::{word_pos}\")\n",
    "            \n",
    "            last_concept = word_pos\n",
    "            last_concept_index = index\n",
    "        \n",
    "    # print(related_concepts)\n",
    "    # print()\n",
    "    \n",
    "#     print('-----')\n",
    "    i = i + 1\n",
    "#     if i == 4:\n",
    "#         break\n",
    "\n",
    "# for concept_relation in concept_relations:\n",
    "#     print(concept_relation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Metrics for term extraction chapter 4 first section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "number_of_manual_concepts: 104\nnumber_of_automatic_concepts: 129\nnumber_of_fully_correct_manual_concepts: 89\nnumber_of_fully_correct_automatic_concepts: 91\nnumber_of_full_and_partial_correct_manual_concepts: 101\nnumber_of_full_and_partial_correct_automatic_concepts: 106\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# automatic_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4/Automated concepts extracted 4.txt\"\n",
    "# manual_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4/Manual concepts extracted 4.txt\"\n",
    "\n",
    "# automatic_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4.2/Automated concepts extracted 4.2.txt\"\n",
    "# manual_concepts_file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Automated concepts extracted/4.2/Manual concepts extracted 4.2.txt\"\n",
    "# \n",
    "# with open(automatic_concepts_file_path, 'r') as file:\n",
    "#     automatic_concepts = file.read()\n",
    "\n",
    "with open(manual_concepts_file_path, 'r') as file:\n",
    "    manual_concepts = file.read()\n",
    "\n",
    "manual_concepts_list = manual_concepts.split('\\n')\n",
    "manual_concepts_list = [x.lower() for x in manual_concepts_list]\n",
    "\n",
    "# print(\"Manual concepts\")\n",
    "# print(list(dict.fromkeys(manual_concepts_list)))\n",
    "# print()\n",
    "\n",
    "# automatic_concepts_list = ['Service Operation', 'processes', 'paragraph', 'detail', 'chapter', 'reference', 'structure', 'processes', 'detail', 'chapter', 'Please note', 'roles', 'process', 'tools', 'process', 'Chapters', 'Management', 'process', 'monitors', 'events', 'IT infrastructure', 'operation', 'exception conditions', 'Incident Management', 'service', 'users', 'order', 'business impact', 'Problem Management', 'root-cause analysis', 'cause', 'events', 'incidents', 'activities', 'problems/incidents', 'Known Error subprocess', 'quicker diagnosis', 'resolution', 'incidents', 'NOTE', 'distinction', 'incidents', 'problems', 'Incident', 'Problem Records', 'danger', 'Incidents', 'support cycle', 'actions', 'recurrence', 'incidents', 'Incidents', 'root cause analysis', 'visibility', 'user ’ s service', 'SLA targets', 'service', 'users', 'expectations', 'results', 'number', 'incidents', '‘ purge ’', 'visibility', 'issues', 'Request Fulfilment', 'management', 'customer', 'user requests', 'incident', 'service delay', 'disruption', 'organizations', 'requests', 'category ’', 'incidents', 'information', 'Incident Management system', 'others', 'volumes', 'business priority', 'requests', 'provision', 'Request Fulfilment', 'Request Fulfilment process', 'practice', 'Request Fulfilment process', 'customer', 'user requests', 'types', 'requests', 'facilities', 'moves', 'supplies', 'IT services', 'requests', 'SLA measures', 'records', 'process flow', 'practice', 'organizations', 'Access Management', 'process', 'users', 'right', 'service', 'access', 'users', 'users', 'ability', 'access services', 'stages', 'resources', 'HR', 'lifecycle', 'Access Management', 'Identity', 'Rights Management', 'organizations', 'addition', 'processes', 'Service Operation', 'phases', 'Service Management Lifecycle', 'aspects', 'processes', 'part', 'chapter', 'include', 'Change Management', 'process', 'Configuration Management', 'Release Management', 'topics', 'Service Transition publication', 'Capacity', 'Availability Management', 'aspects', 'publication', 'detail', 'Service Design publication', 'Financial Management', 'Service Strategy publication', 'Knowledge Management', 'Service Transition publication', 'IT Service Continuity', 'Service Design publication', 'Service Reporting', 'Measurement', 'Continual Service Improvement publication']\n",
    "automatic_concepts_list = all_noun_phrases\n",
    "automatic_concepts_list = [x.lower() for x in automatic_concepts_list]\n",
    "\n",
    "# print(\"all noun phrases\")\n",
    "# print(list(dict.fromkeys(automatic_concepts_list)))\n",
    "\n",
    "count = 0\n",
    "for concept in manual_concepts_list:\n",
    "    if concept in automatic_concepts_list:\n",
    "        count = count + 1\n",
    "\n",
    "number_of_fully_correct_manual_concepts = count\n",
    "\n",
    "number_of_manual_concepts = len(manual_concepts_list)\n",
    "\n",
    "count = 0\n",
    "for concept in automatic_concepts_list:\n",
    "    if concept in manual_concepts_list:\n",
    "        count = count + 1\n",
    "    \n",
    "number_of_fully_correct_automatic_concepts = count\n",
    "\n",
    "number_of_automatic_concepts = len(automatic_concepts_list)\n",
    "\n",
    "print(f\"number_of_manual_concepts: {number_of_manual_concepts}\")\n",
    "print(f\"number_of_automatic_concepts: {number_of_automatic_concepts}\")\n",
    "print(f\"number_of_fully_correct_manual_concepts: {number_of_fully_correct_manual_concepts}\")\n",
    "print(f\"number_of_fully_correct_automatic_concepts: {number_of_fully_correct_automatic_concepts}\")\n",
    "\n",
    "# Lists to words for partial matches\n",
    "automatic_concepts_list_single_words = [x.split() for x in automatic_concepts_list]\n",
    "# print(automatic_concepts_list_single_words)\n",
    "\n",
    "manual_concepts_list_single_words = [x.split() for x in manual_concepts_list]\n",
    "# print(manual_concepts_list_single_words)\n",
    "\n",
    "count = 0\n",
    "for concept in manual_concepts_list_single_words:\n",
    "    for word in concept:\n",
    "        if word in ' '.join(automatic_concepts_list).split():\n",
    "            count = count + 1\n",
    "            break\n",
    "        \n",
    "number_of_full_and_partial_correct_manual_concepts = count\n",
    "print(f\"number_of_full_and_partial_correct_manual_concepts: {number_of_full_and_partial_correct_manual_concepts}\")\n",
    "\n",
    "count = 0\n",
    "for concept in automatic_concepts_list_single_words:\n",
    "    for word in concept:\n",
    "        if word in ' '.join(manual_concepts_list).split():\n",
    "            count = count + 1\n",
    "            break\n",
    "            \n",
    "number_of_full_and_partial_correct_automatic_concepts = count\n",
    "print(f\"number_of_full_and_partial_correct_automatic_concepts: {number_of_full_and_partial_correct_automatic_concepts}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}