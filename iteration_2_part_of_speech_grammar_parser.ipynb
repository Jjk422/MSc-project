{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part of speech grammer parser\n",
    "Use template to grab words that fit with the part of speech grammar pattern.  \n",
    "Examples of the format of the grammar parser are:  \n",
    "* `<DT><NN><VB><NN>`\n",
    " * Returns `<DT><NN><VBZ><NN>`\n",
    " * e.g. The cat plays piano\n",
    "* `!<DT><NN><VB><NN>`\n",
    " * Returns `<NN><VBZ><NN>`\n",
    " * e.g. cat plays piano\n",
    "* `<DT><NN>*<NN>`\n",
    " * Returns `<DT><NN><VBZ><NN>`\n",
    " * e.g. The cat plays piano\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[('Event', 'JJ'), ('Management', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('that', 'IN'), ('monitors', 'NNS'), ('all', 'DT'), ('events', 'NNS'), ('that', 'WDT'), ('occur', 'VBP'), ('through', 'IN'), ('the', 'DT'), ('IT', 'NNP'), ('infrastructure', 'NN'), ('to', 'TO'), ('allow', 'VB'), ('for', 'IN'), ('normal', 'JJ'), ('operation', 'NN'), ('and', 'CC'), ('also', 'RB'), ('to', 'TO'), ('detect', 'VB'), ('and', 'CC'), ('escalate', 'VB'), ('exception', 'NN'), ('conditions', 'NNS'), ('.', '.')]\n----------------------------------------------------------------------------------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import nltk\n",
    "# sentence = \"The cat plays piano.\"\n",
    "sentence = \"Event Management is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "### Part of speech tagging ###\n",
    "part_of_speech_array = nltk.pos_tag(tokens)\n",
    "print(part_of_speech_array)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# def search_pos_sentence_with_grammar(grammar, sentence, ordering):\n",
    "#     tags_to_search_for = []\n",
    "#     for all_pos_tags in grammar:\n",
    "#         all_pos_tags = all_pos_tags.split(\"/\")\n",
    "#         # print(all_pos_tags)\n",
    "#         tags_to_search_for.append(all_pos_tags)\n",
    "#         \n",
    "#     print(tags_to_search_for)\n",
    "#     \n",
    "#     phrase = []\n",
    "#     found_start_word = False\n",
    "#     tag_count = 0\n",
    "#     word_count = 0\n",
    "#     for pos_word in sentence:\n",
    "#         if tag_count < len(tags_to_search_for) and pos_word[1] in tags_to_search_for[tag_count]:\n",
    "#             if ordering is True:\n",
    "#                 \n",
    "#             tag_count = tag_count + 1\n",
    "#             phrase.append(pos_word)\n",
    "#         word_count = word_count + 1\n",
    "#     print(phrase)\n",
    "#     return phrase\n",
    "#         \n",
    "#         \n",
    "#     \n",
    "#         \n",
    "# def pos_grammar_parse(grammar, sentence, ordering=False):\n",
    "#     grammar = [item.replace(\"<\", \"\").upper() for item in grammar.split(\">\") if item is not '']\n",
    "#     # grammar = [item.split(\"/\") for item in grammar]\n",
    "#     # print(grammar)\n",
    "#     return search_pos_sentence_with_grammar(grammar, sentence, ordering)\n",
    "#     # print(grammar)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[('Event', 'JJ'), ('Management', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('that', 'IN'), ('monitor', 'NNS'), ('all', 'DT'), ('event', 'NNS'), ('that', 'WDT'), ('occur', 'VBP'), ('through', 'IN'), ('the', 'DT'), ('IT', 'NNP'), ('infrastructure', 'NN'), ('to', 'TO'), ('allow', 'VB'), ('for', 'IN'), ('normal', 'JJ'), ('operation', 'NN'), ('and', 'CC'), ('also', 'RB'), ('to', 'TO'), ('detect', 'VB'), ('and', 'CC'), ('escalate', 'VB'), ('exception', 'NN'), ('condition', 'NNS'), ('.', '.')]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "part_of_speech_array_lemmatized = []\n",
    "\n",
    "for part_of_speech in part_of_speech_array:\n",
    "    part_of_speech_array_lemmatized.append(\n",
    "        (lemmatizer.lemmatize(part_of_speech[0]), part_of_speech[1])\n",
    "    ) \n",
    "\n",
    "print(part_of_speech_array_lemmatized)\n",
    "\n",
    "# print(part_of_speech_array)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[('Event', 'JJ'), ('Management', 'NNP'), ('process', 'NN'), ('monitor', 'NNS'), ('event', 'NNS'), ('occur', 'VBP'), ('IT', 'NNP'), ('infrastructure', 'NN'), ('allow', 'VB'), ('normal', 'JJ'), ('operation', 'NN'), ('also', 'RB'), ('detect', 'VB'), ('escalate', 'VB'), ('exception', 'NN'), ('condition', 'NNS'), ('.', '.')]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "part_of_speech_array_lemmatized_no_stopwords = [word_pos for word_pos in part_of_speech_array_lemmatized if not word_pos[0] in stop_words]\n",
    "\n",
    "print(part_of_speech_array_lemmatized_no_stopwords)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0, 2, 4, 5, 7, 9, 14]\n{'Event Management': ['is::process'], 'process': ['that::monitor'], 'monitor': ['::event'], 'event': ['occur through::IT infrastructure']}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# noun_phrase_grammar = \"NP: {<DT>?<JJ>*<NN|NNP|NNS>*}\"\n",
    "# noun_phrase_grammar = \"NP: {<JJ>*<NN|NNP|NNS>*}\"\n",
    "noun_phrase_grammar = \"NP: {<JJ>*<NN|NNP|NNS>+}\"\n",
    "# print(pos_grammar_parse(grammar, part_of_speech_array))\n",
    "\n",
    "regex = nltk.RegexpParser(noun_phrase_grammar)\n",
    "noun_phrases_chunked = regex.parse(part_of_speech_array_lemmatized)\n",
    "noun_phrases = [item for item in noun_phrases_chunked.subtrees() if item.label() == \"NP\"]\n",
    "\n",
    "# print(noun_phrases)\n",
    "\n",
    "# for item in noun_phrases_chunked:\n",
    "#     print(item)\n",
    "# print()\n",
    "# \n",
    "# for concept in noun_phrases:\n",
    "#     print(concept)\n",
    "#     \n",
    "# print()\n",
    "\n",
    "# relation_grammer = \"\"\"\n",
    "#     Relation: {<NP>?.*?<VBZ|VBP|IN|VB>+.*?<NP>?}\n",
    "#     \"\"\"\n",
    "\n",
    "relation_grammer = \"\"\"\n",
    "    Relation: {.*?<VBZ|VBP|IN|VB>+.*?}\n",
    "    \"\"\"\n",
    "regex = nltk.RegexpParser(relation_grammer)\n",
    "parser = regex.parse(noun_phrases_chunked)\n",
    "# relation_phrases = [item for item in parser.subtrees() if item.label() == \"Relation\"]\n",
    "# \n",
    "# # print(relation_phrases)\n",
    "# \n",
    "# # for parse in parser:\n",
    "# #     print(parse)\n",
    "# \n",
    "# relation_new = []\n",
    "# # TODO: Find a less hacky way to do this\n",
    "# for index, relation in enumerate(relation_phrases):\n",
    "#     # print(relation)\n",
    "# \n",
    "#     temp_relation = []\n",
    "#     if index is not 0:\n",
    "#         temp_relation.append(relation_phrases)\n",
    "#         temp_relation.append(item for item in relation)\n",
    "#         # print(f\"TEMP RELATION: {temp_relation}\")\n",
    "#         relation = temp_relation\n",
    "# \n",
    "#     # print(temp_relation)\n",
    "#     relation_new.append(relation)\n",
    "#     # print(relation_new)\n",
    "# \n",
    "# print()\n",
    "# \n",
    "# # for relation in relation_new:\n",
    "# #     for item in relation:\n",
    "# #         print(item)\n",
    "#     \n",
    "# for relation in relation_phrases:\n",
    "#     print(relation)\n",
    "\n",
    "# for element in parser.subtrees():\n",
    "#     if element.label() in ['NP', 'Relation']:\n",
    "#         print(element)\n",
    "        \n",
    "# Chunk conjunctives\n",
    "conjunctive_grammer = \"Conjunctive: {.*?<CC>+.*?}\"\n",
    "regex = nltk.RegexpParser(conjunctive_grammer)\n",
    "parser = regex.parse(parser)\n",
    "\n",
    "# print(parser)\n",
    "\n",
    "# for element in parser.subtrees():\n",
    "#     if element.label() in ['NP', 'Relation', 'Conjunctive']:\n",
    "#         print(element)\n",
    "\n",
    "relations = [element for element in parser.subtrees() if element.label() in ['NP', 'Relation', 'Conjunctive']]\n",
    "\n",
    "noun_phrase_indexes = [index for index, element in enumerate(relations) if element.label() == \"NP\"]\n",
    "\n",
    "print(noun_phrase_indexes)\n",
    "\n",
    "relationships = []\n",
    "for index, element in enumerate(noun_phrase_indexes[::2]):\n",
    "    relationships.append(relations[noun_phrase_indexes[index]:noun_phrase_indexes[index + 1] + 1:])\n",
    "\n",
    "ontology = {}\n",
    "    \n",
    "for relationship in relationships:\n",
    "    temp_relation = []\n",
    "    \n",
    "    # for element in relationship[1::]:\n",
    "    #     if element.label() == \"NP\":\n",
    "    #         temp_relation.append(\"::\")\n",
    "    #         \n",
    "    #     # TODO: Fix the below line, only takes first word in relation and related to element\n",
    "    #     temp_relation.append(\" \".join([i[0] for i in element.leaves()]))\n",
    "    #     print(temp_relation)\n",
    "    # \n",
    "    #     # print(relationship[0].leaves()[0][0])\n",
    "    #     \n",
    "    #     # print(temp_relation)\n",
    "    #     # print(\" \".join(temp_relation))\n",
    "    \n",
    "    # for i in relationship[1::]:\n",
    "    #     # print(i)\n",
    "    #     for j in i.leaves():\n",
    "    #         print(j[0])\n",
    "    \n",
    "    first_concept = \" \".join([i[0] for i in relationship[0].leaves()])\n",
    "    \n",
    "    rel = \" \".join(\" \".join([j[0] for j in i.leaves()]) for i in relationship[1::] if i.label() != \"NP\")\n",
    "    rel = f\"{rel}::{' '.join([i[0] for i in relationship[-1].leaves()])}\"\n",
    "    \n",
    "    if first_concept not in ontology.keys():\n",
    "        ontology[first_concept] = []\n",
    "    ontology[first_concept].append(rel)\n",
    "    # print(\"-\" * 100)\n",
    "\n",
    "    # first_concept = \" \".join([i[0] for i in relationship[0].leaves()])\n",
    "    # # print(first_concept)\n",
    "    # \n",
    "    # if first_concept not in ontology.keys():\n",
    "    #     ontology[first_concept] = []\n",
    "    # ontology[first_concept].append(\" \".join(temp_relation))\n",
    "    # # print(\"-\" * 100)\n",
    "    \n",
    "print(ontology)\n",
    "\n",
    "# relations_with_noun_phrases = []\n",
    "# for index, element in enumerate(relations):\n",
    "#     if element.label() == 'NP':\n",
    "#         for elementSub in relations[index::]:\n",
    "#             print(elementSub)\n",
    "#             \n",
    "# \n",
    "# # print(relations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}