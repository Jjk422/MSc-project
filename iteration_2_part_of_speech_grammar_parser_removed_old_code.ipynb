{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part of speech grammer parser\n",
    "Use template to grab words that fit with the part of speech grammar pattern.  \n",
    "Examples of the format of the grammar parser are:  \n",
    "* `<DT><NN><VB><NN>`\n",
    " * Returns `<DT><NN><VBZ><NN>`\n",
    " * e.g. The cat plays piano\n",
    "* `!<DT><NN><VB><NN>`\n",
    " * Returns `<NN><VBZ><NN>`\n",
    " * e.g. cat plays piano\n",
    "* `<DT><NN>*<NN>`\n",
    " * Returns `<DT><NN><VBZ><NN>`\n",
    " * e.g. The cat plays piano\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create ontology no methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "ontology_main: {}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import nltk\n",
    "# sentence = \"The cat plays piano.\"\n",
    "\n",
    "file_path = \"ITIL Books\\ITIL 3\\\\Service operation pdfminer extracted\\\\ITIL3 Service Operation pdfminer extract test.txt\"\n",
    "# file_path = \"ITIL Books/ITIL 3/Service operation chapter 4/Service operation chapter 4 - 4.txt\"\n",
    "\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    file_contents = file.read()\n",
    "    \n",
    "sentences = nltk.tokenize.sent_tokenize(file_contents)\n",
    "\n",
    "sentence = \"Event Management is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\"\n",
    "\n",
    "ontology_main = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    ontology = {}\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    ### Part of speech tagging ###\n",
    "    part_of_speech_array = nltk.pos_tag(tokens)\n",
    "    # print(part_of_speech_array)\n",
    "    # print(\"-\" * 100)\n",
    "    \n",
    "    # def search_pos_sentence_with_grammar(grammar, sentence, ordering):\n",
    "    #     tags_to_search_for = []\n",
    "    #     for all_pos_tags in grammar:\n",
    "    #         all_pos_tags = all_pos_tags.split(\"/\")\n",
    "    #         # print(all_pos_tags)\n",
    "    #         tags_to_search_for.append(all_pos_tags)\n",
    "    #         \n",
    "    #     print(tags_to_search_for)\n",
    "    #     \n",
    "    #     phrase = []\n",
    "    #     found_start_word = False\n",
    "    #     tag_count = 0\n",
    "    #     word_count = 0\n",
    "    #     for pos_word in sentence:\n",
    "    #         if tag_count < len(tags_to_search_for) and pos_word[1] in tags_to_search_for[tag_count]:\n",
    "    #             if ordering is True:\n",
    "    #                 \n",
    "    #             tag_count = tag_count + 1\n",
    "    #             phrase.append(pos_word)\n",
    "    #         word_count = word_count + 1\n",
    "    #     print(phrase)\n",
    "    #     return phrase\n",
    "    #         \n",
    "    #         \n",
    "    #     \n",
    "    #         \n",
    "    # def pos_grammar_parse(grammar, sentence, ordering=False):\n",
    "    #     grammar = [item.replace(\"<\", \"\").upper() for item in grammar.split(\">\") if item is not '']\n",
    "    #     # grammar = [item.split(\"/\") for item in grammar]\n",
    "    #     # print(grammar)\n",
    "    #     return search_pos_sentence_with_grammar(grammar, sentence, ordering)\n",
    "    #     # print(grammar)\n",
    "    \n",
    "    #%%\n",
    "    from nltk.stem import WordNetLemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    part_of_speech_array_lemmatized = []\n",
    "    \n",
    "    for part_of_speech in part_of_speech_array:\n",
    "        part_of_speech_array_lemmatized.append(\n",
    "            (lemmatizer.lemmatize(part_of_speech[0]), part_of_speech[1])\n",
    "        ) \n",
    "    \n",
    "    # print(part_of_speech_array_lemmatized)\n",
    "    \n",
    "    # print(part_of_speech_array)\n",
    "    \n",
    "    #%%\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    part_of_speech_array_lemmatized_no_stopwords = [word_pos for word_pos in part_of_speech_array_lemmatized if not word_pos[0] in stop_words]\n",
    "    \n",
    "    # print(part_of_speech_array_lemmatized_no_stopwords)\n",
    "    \n",
    "    #%%\n",
    "    # noun_phrase_grammar = \"NP: {<DT>?<JJ>*<NN|NNP|NNS>*}\"\n",
    "    # noun_phrase_grammar = \"NP: {<JJ>*<NN|NNP|NNS>*}\"\n",
    "    # noun_phrase_grammar = \"NP: {<JJ>*<NN|NNP|NNS>+}\"\n",
    "    noun_phrase_grammar = \"NP: {<JJ>*<NN|NNP|NNS>+}\"\n",
    "    # print(pos_grammar_parse(grammar, part_of_speech_array))\n",
    "    \n",
    "    regex = nltk.RegexpParser(noun_phrase_grammar)\n",
    "    noun_phrases_chunked = regex.parse(part_of_speech_array_lemmatized)\n",
    "    noun_phrases = [item for item in noun_phrases_chunked.subtrees() if item.label() == \"NP\"]\n",
    "    \n",
    "    # print(noun_phrases)\n",
    "    \n",
    "    # for item in noun_phrases_chunked:\n",
    "    #     print(item)\n",
    "    # print()\n",
    "    # \n",
    "    # for concept in noun_phrases:\n",
    "    #     print(concept)\n",
    "    #     \n",
    "    # print()\n",
    "    \n",
    "    # relation_grammer = \"\"\"\n",
    "    #     Relation: {<NP>?.*?<VBZ|VBP|IN|VB>+.*?<NP>?}\n",
    "    #     \"\"\"\n",
    "    \n",
    "    relation_grammer = \"\"\"\n",
    "        Relation: {.*?<VBZ|VBP|IN|VB>+.*?}\n",
    "        \"\"\"\n",
    "    regex = nltk.RegexpParser(relation_grammer)\n",
    "    parser = regex.parse(noun_phrases_chunked)\n",
    "    # relation_phrases = [item for item in parser.subtrees() if item.label() == \"Relation\"]\n",
    "    # \n",
    "    # # print(relation_phrases)\n",
    "    # \n",
    "    # # for parse in parser:\n",
    "    # #     print(parse)\n",
    "    # \n",
    "    # relation_new = []\n",
    "    # # TODO: Find a less hacky way to do this\n",
    "    # for index, relation in enumerate(relation_phrases):\n",
    "    #     # print(relation)\n",
    "    # \n",
    "    #     temp_relation = []\n",
    "    #     if index is not 0:\n",
    "    #         temp_relation.append(relation_phrases)\n",
    "    #         temp_relation.append(item for item in relation)\n",
    "    #         # print(f\"TEMP RELATION: {temp_relation}\")\n",
    "    #         relation = temp_relation\n",
    "    # \n",
    "    #     # print(temp_relation)\n",
    "    #     relation_new.append(relation)\n",
    "    #     # print(relation_new)\n",
    "    # \n",
    "    # print()\n",
    "    # \n",
    "    # # for relation in relation_new:\n",
    "    # #     for item in relation:\n",
    "    # #         print(item)\n",
    "    #     \n",
    "    # for relation in relation_phrases:\n",
    "    #     print(relation)\n",
    "    \n",
    "    # for element in parser.subtrees():\n",
    "    #     if element.label() in ['NP', 'Relation']:\n",
    "    #         print(element)\n",
    "            \n",
    "    # Chunk conjunctives\n",
    "    conjunctive_grammer = \"Conjunctive: {.*?<CC>+.*?}\"\n",
    "    regex = nltk.RegexpParser(conjunctive_grammer)\n",
    "    parser = regex.parse(parser)\n",
    "    \n",
    "    # print(parser)\n",
    "    \n",
    "    # for element in parser.subtrees():\n",
    "    #     if element.label() in ['NP', 'Relation', 'Conjunctive']:\n",
    "    #         print(element)\n",
    "    \n",
    "    relations = [element for element in parser.subtrees() if element.label() in ['NP', 'Relation', 'Conjunctive']]\n",
    "    \n",
    "    noun_phrase_indexes = [index for index, element in enumerate(relations) if element.label() == \"NP\"]\n",
    "    \n",
    "    # print(noun_phrase_indexes)\n",
    "    \n",
    "    relationships = []\n",
    "    for index, element in enumerate(noun_phrase_indexes[::2]):\n",
    "        try:\n",
    "            relationships.append(relations[noun_phrase_indexes[index]:noun_phrase_indexes[index + 1] + 1:])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    # ontology = {}\n",
    "        \n",
    "    # print(sentence)    \n",
    "    \n",
    "    for relationship in relationships:\n",
    "        conjunctive_relation = []\n",
    "        temp_relation = []\n",
    "        \n",
    "        first_concept = \" \".join([i[0] for i in relationship[0].leaves()])\n",
    "        \n",
    "        # print(first_concept.replace(\"Â¦ \", \"\"))\n",
    "        \n",
    "        # TODO: Make this work better make conjuctive relation addative\n",
    "        # In other words make conjunctive into [[]] with words added to [[\"is\", \"here\"]]\n",
    "        # then use conjunctive to add new value so [[\"is\", \"here\"], [\"is\", \"an\", \"animal\"]]\n",
    "        for element in relationship[1::]:\n",
    "            if element.label() == \"Relation\":\n",
    "                temp_relation.append(\" \".join(i[0] for i in element.leaves()))\n",
    "                for conjunctive_list in conjunctive_relation[::]:\n",
    "                    conjunctive_relation[-1].append(\" \".join(i[0] for i in element.leaves()))\n",
    "            elif element.label() == \"Conjunctive\":\n",
    "                conjunctive_relation.append(temp_relation)\n",
    "                conjunctive_relation.append(temp_relation[:-1:])\n",
    "                \n",
    "                # for conjuctive_list in conjunctive_relation:\n",
    "                # temp_relation.append(\" \".join(i[0] for i in element.leaves()))\n",
    "                # conjunctive_relation.append(temp_relation)\n",
    "                # conjunctive_relation.append(temp_relation[:-1:].append(\" \".join(i[0] for i in element.leaves())))\n",
    "                \n",
    "            elif element.label() == \"NP\":\n",
    "                temp_relation.append(\"::\" + \" \".join(i[0] for i in element.leaves()))\n",
    "                for conjuctive_list in conjunctive_relation:\n",
    "                    conjunctive_list.append(\"::\" + \" \".join(i[0] for i in element.leaves()))\n",
    "            # print(element)\n",
    "            \n",
    "            # print(element)\n",
    "        # print(temp_relation)\n",
    "        # print(f\"conjunctive_relation: {conjunctive_relation}\")\n",
    "\n",
    "        # print(\"-\" * 100)\n",
    "            \n",
    "        # first_concept = \" \".join([i[0] for i in relationship[0].leaves()])\n",
    "        # \n",
    "        # rel = \" \".join(\" \".join([j[0] for j in i.leaves()]) for i in relationship[1::] if i.label() != \"NP\")\n",
    "        # rel = f\"{rel}::{' '.join([i[0] for i in relationship[-1].leaves()])}\"\n",
    "        # \n",
    "        # if first_concept not in ontology.keys():\n",
    "        #     ontology[first_concept] = []\n",
    "        # ontology[first_concept].append(rel)\n",
    "        \n",
    "        # print(f\"ontology: {ontology}\")\n",
    "    \n",
    "        for element, value in ontology:\n",
    "            ontology_main[element] = ontology_main[element].append[value.flatten]\n",
    "        \n",
    "# # Print ontology\n",
    "print(f\"ontology_main: {ontology_main}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ontology code moved into methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "cat\n[['play::piano']]\n----------------------------------------------------------------------------------------------------\nEvent Management process\n[['monitor::event']]\n----------------------------------------------------------------------------------------------------\nevent\n[['occur::IT infrastructure']]\n----------------------------------------------------------------------------------------------------\nIT infrastructure\n[['allow::normal operation']]\n----------------------------------------------------------------------------------------------------\nPeople\n[['use::book scroll']]\n----------------------------------------------------------------------------------------------------\nHumans\n[['use::book']]\n----------------------------------------------------------------------------------------------------\nbook\n[['walk::road']]\n----------------------------------------------------------------------------------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Methods for non conjunctive relation selections (ises code from iteration_2_part_of_speech_grammar_parser)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "# sentence = \"The cat plays piano.\"\n",
    "# sentence = \"Event Management is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\"\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "    \n",
    "def lemmatize_tokens(part_of_speech_array):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    part_of_speech_array_lemmatized = []\n",
    "    \n",
    "    for part_of_speech in part_of_speech_array:\n",
    "        part_of_speech_array_lemmatized.append(\n",
    "            (lemmatizer.lemmatize(part_of_speech[0]), part_of_speech[1])\n",
    "        )\n",
    "    return part_of_speech_array_lemmatized\n",
    "\n",
    "def remove_stopwords_from_tokens(part_of_speech_array_lemmatized):\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    part_of_speech_array_lemmatized_no_stopwords = [word_pos for word_pos in part_of_speech_array_lemmatized if not word_pos[0] in stop_words]\n",
    "    \n",
    "    return part_of_speech_array_lemmatized_no_stopwords\n",
    "\n",
    "def get_noun_phrases(part_of_speech_array_lemmatized, noun_phrase_grammar_input=\"\"):\n",
    "    if noun_phrase_grammar_input == \"\":\n",
    "        noun_phrase_grammar = \"NP: {<JJ>*<NN|NNP|NNS>+}\"\n",
    "    else:\n",
    "        noun_phrase_grammar = noun_phrase_grammar_input\n",
    "    \n",
    "    regex = nltk.RegexpParser(noun_phrase_grammar)\n",
    "    noun_phrases_chunked = regex.parse(part_of_speech_array_lemmatized)\n",
    "    noun_phrases = [item for item in noun_phrases_chunked.subtrees() if item.label() == \"NP\"]\n",
    "    \n",
    "    return noun_phrases_chunked, noun_phrases\n",
    "\n",
    "def get_relations(noun_phrases_chunked):\n",
    "    relation_grammer = \"\"\"\n",
    "    Relation: {.*?<VBZ|VBP|IN|VB>+.*?}\n",
    "    \"\"\"\n",
    "    regex = nltk.RegexpParser(relation_grammer)\n",
    "    parser = regex.parse(noun_phrases_chunked)\n",
    "            \n",
    "    # Chunk conjunctives\n",
    "    conjunctive_grammer = \"Conjunctive: {.*?<CC>+.*?}\"\n",
    "    regex = nltk.RegexpParser(conjunctive_grammer)\n",
    "    parser = regex.parse(parser)\n",
    "    \n",
    "    relations = [element for element in parser.subtrees() if element.label() in ['NP', 'Relation', 'Conjunctive']]\n",
    "    \n",
    "    noun_phrase_indexes = [index for index, element in enumerate(relations) if element.label() == \"NP\"]\n",
    "        \n",
    "    relationships = []\n",
    "    for index, element in enumerate(noun_phrase_indexes[::2]):\n",
    "        try:\n",
    "            relationships.append(relations[noun_phrase_indexes[index]:noun_phrase_indexes[index + 1] + 1:])\n",
    "        except IndexError:\n",
    "            continue\n",
    "        \n",
    "    return relationships\n",
    "\n",
    "\n",
    "def create_ontology(relationships):    \n",
    "    ontology = {}\n",
    "        \n",
    "    for relationship in relationships:\n",
    "        temp_relation = []\n",
    "        \n",
    "        first_concept = \" \".join([i[0] for i in relationship[0].leaves()])\n",
    "        \n",
    "        rel = \" \".join(\" \".join([j[0] for j in i.leaves()]) for i in relationship[1::] if i.label() != \"NP\")\n",
    "        rel = f\"{rel}::{' '.join([i[0] for i in relationship[-1].leaves()])}\"\n",
    "        \n",
    "        if first_concept not in ontology.keys():\n",
    "            ontology[first_concept] = []\n",
    "        ontology[first_concept].append(rel)\n",
    "        \n",
    "    return ontology\n",
    "\n",
    "# Test sentences for comparison results\n",
    "sentences = [\n",
    "    \"The cat plays piano.\",\n",
    "    \"Event Management is a process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\",\n",
    "    \"People use books and scrolls.\",\n",
    "    \"Humans use books and also walk on roads\"\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "ontology_main = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenised_sentence = tokenize_sentence(sentence)\n",
    "    lemmatized_tokens = lemmatize_tokens(tokenised_sentence)\n",
    "    tokens_with_removed_stopwords = remove_stopwords_from_tokens(lemmatized_tokens)\n",
    "    noun_phrases_chunked, noun_phrases = get_noun_phrases(tokens_with_removed_stopwords)\n",
    "    relationships = get_relations(noun_phrases_chunked)\n",
    "    ontology = create_ontology(relationships)\n",
    "    \n",
    "    # print(ontology)\n",
    "    for key, value in ontology.items():\n",
    "        ontology[key] = [i for i in value if re.search(\".+::.+\", i)]\n",
    "        \n",
    "        if key not in ontology_main.keys():\n",
    "            ontology_main[key] = []            \n",
    "        ontology_main[key].append(value)\n",
    "    \n",
    "# print(ontology_main)\n",
    "\n",
    "for key, value in ontology_main.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print(\"-\" * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Xml output of file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\" ?><root><cat type=\"list\"><item type=\"list\"><item type=\"str\">play::piano</item></item></cat><Event_Management_process type=\"list\"><item type=\"list\"><item type=\"str\">monitor::event</item></item></Event_Management_process><event type=\"list\"><item type=\"list\"><item type=\"str\">occur::IT infrastructure</item></item></event><IT_infrastructure type=\"list\"><item type=\"list\"><item type=\"str\">allow::normal operation</item></item></IT_infrastructure><People type=\"list\"><item type=\"list\"><item type=\"str\">use::book scroll</item></item></People><Humans type=\"list\"><item type=\"list\"><item type=\"str\">use::book</item></item></Humans><book type=\"list\"><item type=\"list\"><item type=\"str\">walk::road</item></item></book></root>'\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import dicttoxml\n",
    "xml = dicttoxml.dicttoxml(ontology_main)\n",
    "print(xml)\n",
    "\n",
    "with open(\"ITIL Books\\\\ITIL 3\\\\Service operation pdfminer extracted\\\\pdfminer concept extraction xml test.xml\", \"wb\") as file:\n",
    "    file.write(xml)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Printing results for chapter section 4\n----------------------------------------------------------------------------------------------------\nnumber_of_manual_concepts: 64\nnumber_of_automatic_concepts: 99\nnumber_of_fully_correct_manual_concepts: 20\nnumber_of_fully_correct_automatic_concepts: 20\nnumber_of_full_and_partial_correct_manual_concepts: 63\nnumber_of_full_and_partial_correct_automatic_concepts: 78\n\nPrinting results for chapter section 4.1\n----------------------------------------------------------------------------------------------------\nnumber_of_manual_concepts: 102\nnumber_of_automatic_concepts: 117\nnumber_of_fully_correct_manual_concepts: 30\nnumber_of_fully_correct_automatic_concepts: 30\nnumber_of_full_and_partial_correct_manual_concepts: 96\nnumber_of_full_and_partial_correct_automatic_concepts: 93\n\nPrinting results for chapter section 4.2\n----------------------------------------------------------------------------------------------------\n",
      "number_of_manual_concepts: 62\nnumber_of_automatic_concepts: 72\nnumber_of_fully_correct_manual_concepts: 18\nnumber_of_fully_correct_automatic_concepts: 18\nnumber_of_full_and_partial_correct_manual_concepts: 58\nnumber_of_full_and_partial_correct_automatic_concepts: 57\n\nPrinting results for chapter section 4.3\n----------------------------------------------------------------------------------------------------\nnumber_of_manual_concepts: 80\nnumber_of_automatic_concepts: 84\nnumber_of_fully_correct_manual_concepts: 25\nnumber_of_fully_correct_automatic_concepts: 25\nnumber_of_full_and_partial_correct_manual_concepts: 72\nnumber_of_full_and_partial_correct_automatic_concepts: 71\n\nPrinting results for chapter section 4.4\n----------------------------------------------------------------------------------------------------\nnumber_of_manual_concepts: 54\nnumber_of_automatic_concepts: 50\nnumber_of_fully_correct_manual_concepts: 12\nnumber_of_fully_correct_automatic_concepts: 12\nnumber_of_full_and_partial_correct_manual_concepts: 49\nnumber_of_full_and_partial_correct_automatic_concepts: 45\n\nPrinting results for chapter section 4.5\n----------------------------------------------------------------------------------------------------\nnumber_of_manual_concepts: 62\nnumber_of_automatic_concepts: 51\nnumber_of_fully_correct_manual_concepts: 19\nnumber_of_fully_correct_automatic_concepts: 19\nnumber_of_full_and_partial_correct_manual_concepts: 52\nnumber_of_full_and_partial_correct_automatic_concepts: 48\n\nPrinting results for chapter section 4.6\n----------------------------------------------------------------------------------------------------\nnumber_of_manual_concepts: 58\nnumber_of_automatic_concepts: 50\nnumber_of_fully_correct_manual_concepts: 15\nnumber_of_fully_correct_automatic_concepts: 15\nnumber_of_full_and_partial_correct_manual_concepts: 46\nnumber_of_full_and_partial_correct_automatic_concepts: 48\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import print_metrics\n",
    "\n",
    "### Variables ###\n",
    "# To lemmatize the arrays\n",
    "lemmatize_pos_array_bool = True\n",
    "# lemmatize_pos_array_bool = False\n",
    "\n",
    "# To remove stopwords from the tokens\n",
    "remove_stopwords_from_tokens_bool = True\n",
    "# remove_stopwords_from_tokens_bool = False\n",
    "\n",
    "# To remove duplicate words or phrases from the arrays \n",
    "remove_duplicates_from_python_lists = True\n",
    "# remove_duplicates_from_python_lists = False\n",
    "\n",
    "\n",
    "# To use debug mode\n",
    "# debug = True\n",
    "debug = False\n",
    "\n",
    "files_to_use_array = [\n",
    "    [\"4\", print_metrics.get_extracted_text_file_path(4), print_metrics.get_manual_concepts_file_path(4)],\n",
    "    [\"4.1\", print_metrics.get_extracted_text_file_path(4.1), print_metrics.get_manual_concepts_file_path(4.1)],\n",
    "    [\"4.2\", print_metrics.get_extracted_text_file_path(4.2), print_metrics.get_manual_concepts_file_path(4.2)],\n",
    "    [\"4.3\", print_metrics.get_extracted_text_file_path(4.3), print_metrics.get_manual_concepts_file_path(4.3)],\n",
    "    [\"4.4\", print_metrics.get_extracted_text_file_path(4.4), print_metrics.get_manual_concepts_file_path(4.4)],\n",
    "    [\"4.5\", print_metrics.get_extracted_text_file_path(4.5), print_metrics.get_manual_concepts_file_path(4.5)],\n",
    "    [\"4.6\", print_metrics.get_extracted_text_file_path(4.6), print_metrics.get_manual_concepts_file_path(4.6)]\n",
    "]\n",
    "# ### Chapter 4 start ###\n",
    "# extracted_text_file_path = print_metrics.get_extracted_text_file_path(4) \n",
    "# manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4)\n",
    "# \n",
    "# # ## Chapter 4 - 4.1 to 4.1.4 ###\n",
    "# extracted_text_file_path = print_metrics.get_extracted_text_file_path(4.1) \n",
    "# manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4.1)\n",
    "# \n",
    "# ### Chapter 4 - 4.2 to 4.2.4 ###\n",
    "# # extracted_text_file_path = print_metrics.get_extracted_text_file_path(4.2) \n",
    "# # manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4.2)\n",
    "# \n",
    "# # ### Chapter 4 - 4.3 to 4.3.4 ###\n",
    "# # extracted_text_file_path = print_metrics.get_extracted_text_file_path(4.3) \n",
    "# # manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4.3)\n",
    "# \n",
    "# # ### Chapter 4 - 4.4 to 4.4.4 ###\n",
    "# # extracted_text_file_path = print_metrics.get_extracted_text_file_path(4.4) \n",
    "# # manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4.4)\n",
    "# \n",
    "# # ### Chapter 4 - 4.5 to 4.5.4 ###\n",
    "# # extracted_text_file_path = print_metrics.get_extracted_text_file_path(4.5) \n",
    "# # manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4.5)\n",
    "# \n",
    "# # ### Chapter 4 - 4.6 to 4.6.4 ###\n",
    "# # extracted_text_file_path = print_metrics.get_extracted_text_file_path(4.6) \n",
    "# # manual_concepts_file_path = print_metrics.get_manual_concepts_file_path(4.6)\n",
    "\n",
    "for files in files_to_use_array:\n",
    "    chapter_title = files[0]\n",
    "    extracted_text_file_path = files[1]\n",
    "    manual_concepts_file_path = files[2]\n",
    "    \n",
    "    print(f\"Printing results for chapter section {chapter_title}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    with open(extracted_text_file_path, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "        \n",
    "    sentences = nltk.tokenize.sent_tokenize(file_contents)\n",
    "    \n",
    "    noun_phrases_list = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenised_sentence = tokenize_sentence(sentence)\n",
    "        lemmatized_tokens = lemmatize_tokens(tokenised_sentence) if lemmatize_pos_array_bool else tokenised_sentence\n",
    "        tokens_with_removed_stopwords = remove_stopwords_from_tokens(lemmatized_tokens) if remove_stopwords_from_tokens_bool else lemmatized_tokens    \n",
    "        noun_phrases_chunked, noun_phrases = get_noun_phrases(tokens_with_removed_stopwords, \"NP: {<JJ>*<NN|NNP|NNS>+}\")\n",
    "        \n",
    "        for noun_phrase in noun_phrases:\n",
    "            temp_noun_phrase = [element[0] for element in noun_phrase]\n",
    "            noun_phrases_list.append(\" \".join(temp_noun_phrase))\n",
    "    \n",
    "    print_metrics.print_metrics(manual_concepts_file_path, noun_phrases_list, lemmatize_first=lemmatize_pos_array_bool, remove_duplicates=remove_duplicates_from_python_lists, debug=debug)\n",
    "        \n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}