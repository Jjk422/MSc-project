{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".",
      "\n",
      "Would you like to download the models for: en_ewt now? (Y/n)",
      "\n",
      "\n",
      "Default download directory: C:\\Users\\User\\stanfordnlp_resources",
      "\n",
      "Hit enter to continue or type an alternate directory.",
      "\n",
      "\n",
      "Downloading models for: en_ewt",
      "\n",
      "Download location: C:\\Users\\User\\stanfordnlp_resources\\en_ewt_models.zip",
      "\n",
      "\n",
      "Download complete.  Models saved to: C:\\Users\\User\\stanfordnlp_resources\\en_ewt_models.zip",
      "\n",
      "Extracting models file for: en_ewt",
      "\n",
      "Cleaning up...",
      "Done.",
      "\n",
      "Use device: cpu",
      "\n",
      "---",
      "\n",
      "Loading: tokenize",
      "\n",
      "With settings: ",
      "\n",
      "{'model_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}",
      "\n",
      "---",
      "\n",
      "Loading: lemma",
      "\n",
      "With settings: ",
      "\n",
      "{'model_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}",
      "\n",
      "Building an attentional Seq2Seq model...",
      "\n",
      "Using a Bi-LSTM encoder",
      "\n",
      "Using soft attention for LSTM.",
      "\n",
      "Finetune all embeddings.",
      "\n",
      "[Running seq2seq lemmatizer with edit classifier]",
      "\n",
      "---",
      "\n",
      "Loading: pos",
      "\n",
      "With settings: ",
      "\n",
      "{'model_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}",
      "\n",
      "Done loading processors!",
      "\n",
      "---",
      "\n",
      "<Token index=1;words=[<Word index=1;text=Event;lemma=event;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=2;words=[<Word index=2;text=Management;lemma=management;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=3;words=[<Word index=3;text=is;lemma=be;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>",
      "\n",
      "<Token index=4;words=[<Word index=4;text=the;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>",
      "\n",
      "<Token index=5;words=[<Word index=5;text=process;lemma=process;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=6;words=[<Word index=6;text=that;lemma=that;upos=PRON;xpos=WDT;feats=PronType=Rel>]>",
      "\n",
      "<Token index=7;words=[<Word index=7;text=monitors;lemma=monitor;upos=VERB;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>",
      "\n",
      "<Token index=8;words=[<Word index=8;text=all;lemma=all;upos=DET;xpos=DT;feats=_>]>",
      "\n",
      "<Token index=9;words=[<Word index=9;text=events;lemma=event;upos=NOUN;xpos=NNS;feats=Number=Plur>]>",
      "\n",
      "<Token index=10;words=[<Word index=10;text=that;lemma=that;upos=PRON;xpos=WDT;feats=PronType=Rel>]>",
      "\n",
      "<Token index=11;words=[<Word index=11;text=occur;lemma=occur;upos=VERB;xpos=VBP;feats=Mood=Ind|Tense=Pres|VerbForm=Fin>]>",
      "\n",
      "<Token index=12;words=[<Word index=12;text=through;lemma=through;upos=ADP;xpos=IN;feats=_>]>",
      "\n",
      "<Token index=13;words=[<Word index=13;text=the;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>",
      "\n",
      "<Token index=14;words=[<Word index=14;text=IT;lemma=it;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=15;words=[<Word index=15;text=infrastructure;lemma=infrastructure;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=16;words=[<Word index=16;text=to;lemma=to;upos=PART;xpos=TO;feats=_>]>",
      "\n",
      "<Token index=17;words=[<Word index=17;text=allow;lemma=allow;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>",
      "\n",
      "<Token index=18;words=[<Word index=18;text=for;lemma=for;upos=ADP;xpos=IN;feats=_>]>",
      "\n",
      "<Token index=19;words=[<Word index=19;text=normal;lemma=normal;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>",
      "\n",
      "<Token index=20;words=[<Word index=20;text=operation;lemma=operation;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=21;words=[<Word index=21;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_>]>",
      "\n",
      "<Token index=22;words=[<Word index=22;text=also;lemma=also;upos=ADV;xpos=RB;feats=_>]>",
      "\n",
      "<Token index=23;words=[<Word index=23;text=to;lemma=to;upos=PART;xpos=TO;feats=_>]>",
      "\n",
      "<Token index=24;words=[<Word index=24;text=detect;lemma=detect;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>",
      "\n",
      "<Token index=25;words=[<Word index=25;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_>]>",
      "\n",
      "<Token index=26;words=[<Word index=26;text=escalate;lemma=escalate;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>",
      "\n",
      "<Token index=27;words=[<Word index=27;text=exception;lemma=exception;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=28;words=[<Word index=28;text=conditions;lemma=condition;upos=NOUN;xpos=NNS;feats=Number=Plur>]>",
      "\n",
      "<Token index=29;words=[<Word index=29;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_>]>",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "\r  0%|          | 0.00/235M [00:00<?, ?B/s]",
      "\r 29%|██▊       | 67.1M/235M [01:10<02:56, 949kB/s]",
      "\r 29%|██▊       | 67.1M/235M [01:30<02:56, 949kB/s]",
      "\r 57%|█████▋    | 134M/235M [01:54<01:33, 1.07MB/s]",
      "\r 57%|█████▋    | 134M/235M [02:10<01:33, 1.07MB/s]",
      "\r 86%|████████▌ | 201M/235M [02:51<00:30, 1.10MB/s]",
      "\r100%|██████████| 235M/235M [03:04<00:00, 1.34MB/s]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')\n",
    "nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos\")\n",
    "doc = nlp(\"Event Management is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\")\n",
    "doc.sentences[0].print_tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "text: Event \tlemma: event\tupos: NOUN\txpos: NN",
      "\n",
      "text: Management \tlemma: management\tupos: NOUN\txpos: NN",
      "\n",
      "text: is \tlemma: be\tupos: AUX\txpos: VBZ",
      "\n",
      "text: the \tlemma: the\tupos: DET\txpos: DT",
      "\n",
      "text: process \tlemma: process\tupos: NOUN\txpos: NN",
      "\n",
      "text: that \tlemma: that\tupos: PRON\txpos: WDT",
      "\n",
      "text: monitors \tlemma: monitor\tupos: VERB\txpos: VBZ",
      "\n",
      "text: all \tlemma: all\tupos: DET\txpos: DT",
      "\n",
      "text: events \tlemma: event\tupos: NOUN\txpos: NNS",
      "\n",
      "text: that \tlemma: that\tupos: PRON\txpos: WDT",
      "\n",
      "text: occur \tlemma: occur\tupos: VERB\txpos: VBP",
      "\n",
      "text: through \tlemma: through\tupos: ADP\txpos: IN",
      "\n",
      "text: the \tlemma: the\tupos: DET\txpos: DT",
      "\n",
      "text: IT \tlemma: it\tupos: NOUN\txpos: NN",
      "\n",
      "text: infrastructure \tlemma: infrastructure\tupos: NOUN\txpos: NN",
      "\n",
      "text: to \tlemma: to\tupos: PART\txpos: TO",
      "\n",
      "text: allow \tlemma: allow\tupos: VERB\txpos: VB",
      "\n",
      "text: for \tlemma: for\tupos: ADP\txpos: IN",
      "\n",
      "text: normal \tlemma: normal\tupos: ADJ\txpos: JJ",
      "\n",
      "text: operation \tlemma: operation\tupos: NOUN\txpos: NN",
      "\n",
      "text: and \tlemma: and\tupos: CCONJ\txpos: CC",
      "\n",
      "text: also \tlemma: also\tupos: ADV\txpos: RB",
      "\n",
      "text: to \tlemma: to\tupos: PART\txpos: TO",
      "\n",
      "text: detect \tlemma: detect\tupos: VERB\txpos: VB",
      "\n",
      "text: and \tlemma: and\tupos: CCONJ\txpos: CC",
      "\n",
      "text: escalate \tlemma: escalate\tupos: VERB\txpos: VB",
      "\n",
      "text: exception \tlemma: exception\tupos: NOUN\txpos: NN",
      "\n",
      "text: conditions \tlemma: condition\tupos: NOUN\txpos: NNS",
      "\n",
      "text: . \tlemma: .\tupos: PUNCT\txpos: .",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import Constants\n",
    "\n",
    "with open(\"ITIL Books/ITIL 3/Service operation glossary/definitions.txt\") as file:\n",
    "    glossary_definitions = file.read()\n",
    "\n",
    "# definitions = [i.lower().replace(\"\\n\", \" \").split('::') for i in glossary_definitions.split('\\n\\n')]\n",
    "definitions = [i.replace(\"\\n\", \" \").split('::') for i in glossary_definitions.split('\\n\\n')]\n",
    "\n",
    "concepts_from_definitions = [i[0].strip() for i in definitions]\n",
    "\n",
    "\n",
    "with open(\"ITIL Books/ITIL 3/Service operation chapter 4/Service operation chapter 4 - all.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "ontology = {}\n",
    "\n",
    "for index in range(0, len(definitions)):\n",
    "    ontology[definitions[index][0].strip()] = [f\"definition::{definitions[index][1].strip()}\"]\n",
    "\n",
    "import nltk\n",
    "# Tokenize into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "# sentences = [nltk.word_tokenize(i.lower().replace(\"\\n\", \" \")) for i in sentences]\n",
    "sentences = [nltk.word_tokenize(i.replace(\"\\n\", \" \")) for i in sentences]\n",
    "\n",
    "sentences_containing_concepts = {}\n",
    "for sentence in sentences:\n",
    "    # print(\"---\")\n",
    "    count = 0\n",
    "    for concept in concepts_from_definitions:\n",
    "        concept_list = concept.split()\n",
    "        \n",
    "        if concept_list[0] in sentence:\n",
    "            add_to_sentences_containing_concepts = True\n",
    "            \n",
    "            if len(concept_list) > 1:\n",
    "                current_word_index = sentence.index(concept_list[0])\n",
    "                concept_word_length_count = 1\n",
    "                for concept_word in concept_list[1::]:\n",
    "                    current_word_index = current_word_index + 1\n",
    "                    \n",
    "                    if sentence[current_word_index] != concept_word:\n",
    "                        add_to_sentences_containing_concepts = False\n",
    "                        break\n",
    "                \n",
    "            if add_to_sentences_containing_concepts is True:\n",
    "                sentence_string = \" \".join(sentence)\n",
    "                if sentence_string not in sentences_containing_concepts.keys():\n",
    "                    sentences_containing_concepts[sentence_string] = []\n",
    "                sentences_containing_concepts[sentence_string].append(concept)\n",
    "            \n",
    "\n",
    "# sentences_containing_concepts = [i for i in sentences if i in concepts_from_definitions]\n",
    " \n",
    "# for sentence in sentences_containing_concepts:\n",
    "#     print(sentence)\n",
    "\n",
    "# print(sentences_containing_concepts)\n",
    "\n",
    "# print(ontology)\n",
    "\n",
    "for sentence, concepts in sentences_containing_concepts.items():\n",
    "    for concept in concepts:\n",
    "        ontology[concept].append(sentence)\n",
    "        \n",
    "# TODO: In definitions replace (book name) with own in book relation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def load_templates(file_path):\n",
    "    with open(file_path) as template_file:\n",
    "        templates = template_file.readlines()\n",
    "    return templates\n",
    "\n",
    "# question_template = \"question_template(Question_number=1,Answer_number=4)::Question->What is the definition for {concept.name}?::Answer->{concept.definition}\"\n",
    "templates = load_templates(\"templates/question_templates.txt\")\n",
    "question_template = templates[random.randint(0, len(templates) - 1)]\n",
    "# question_template = templates[1]\n",
    "\n",
    "# print(templates)\n",
    "# print(question_template)\n",
    "\n",
    "# nltk.tokenize.mwe \n",
    "        \n",
    "def get_definition(concept_name):\n",
    "    return [i.split(\"::\")[1] for i in ontology[concept_name] if i.split(\"::\")[0] == \"definition\"][0]\n",
    "\n",
    "class Concept:\n",
    "    def __init__(self, name, ontology):\n",
    "        self.name = name\n",
    "        self.ontology = ontology\n",
    "        self.attributes = ontology[self.name]\n",
    "        self.definition = get_definition(self.name)\n",
    "        \n",
    "        self.book = re.findall(\"^\\w*\\(.*?\\)\", self.definition)\n",
    "        for book in self.book:\n",
    "            self.definition = self.definition.replace(book, \"\")\n",
    "        self.book = [i.replace(\"(\", \"\").replace(\")\", \"\") for i in self.book]\n",
    "        \n",
    "        self.attributes[0] = f\"definition::{self.definition}\"\n",
    "        # TODO: Fix book\n",
    "\n",
    "class Question:\n",
    "    def __init__(self, question_string, answer_list, question_params):\n",
    "        self.question_string = question_string\n",
    "        self.answer_list = answer_list\n",
    "        self.parameters = question_params\n",
    "        self.correct_answer = answer_list[0]\n",
    "        \n",
    "    def randomise_answers(self):\n",
    "        # self.correct_answer = self.answer_list[0]\n",
    "        random.shuffle(self.answer_list)\n",
    "        return self.answer_list.index(self.correct_answer)\n",
    "    \n",
    "    def print_question(self):\n",
    "        print(self.question_string)\n",
    "        \n",
    "    def print_answers(self):\n",
    "        for answer_index, answer in enumerate(self.answer_list):\n",
    "            print(f\"- {answer_index + 1}: \" + answer.strip(\"\\n\"))\n",
    "    \n",
    "    # def print_question_and_answer(self):        \n",
    "    #     print(self.question_string)\n",
    "    #     self.randomise_answers()\n",
    "    #     for answer in self.answer_list:\n",
    "    #         print(f\"- \" + answer.strip(\"\\n\"))\n",
    "    #     print()\n",
    "        \n",
    "    def format_output_print(self):\n",
    "        self.print_question()\n",
    "        self.randomise_answers()\n",
    "        self.print_answers()\n",
    "        print()\n",
    "        \n",
    "    def format_output_interactive(self):\n",
    "        self.print_question()\n",
    "        correct_answer_index = self.randomise_answers()\n",
    "        self.print_answers()\n",
    "        \n",
    "        while(True):\n",
    "            try:\n",
    "                answer_input = int(input(\"What is the answer (answer ID number) [0 for exit]?\"))\n",
    "                print(answer_input)\n",
    "                if answer_input == 0:\n",
    "                    break\n",
    "                elif 0 < answer_input <= len(self.answer_list):\n",
    "                    if answer_input == (correct_answer_index + 1):\n",
    "                        print(f\"Correct\")\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Incorrect\\nThe correct answer is {self.answer_list[correct_answer_index]}\")\n",
    "                else:\n",
    "                    print(f\"Answer ID [{answer_input}] is not recognised\")\n",
    "            except (ValueError):\n",
    "                print(f\"The inputted answer is not a question number, please enter the answer ID (e.g. 1)\")\n",
    "                \n",
    "            print()\n",
    "                \n",
    "        \n",
    "    def format_output(self, output_type=\"print\"):\n",
    "        if output_type.lower() == \"print\":\n",
    "            self.format_output_print()\n",
    "        elif output_type.lower() == \"interactive\":\n",
    "            self.format_output_interactive()\n",
    "        \n",
    "        \n",
    "def get_incorrect_answers(correct_concept, question_parameters):\n",
    "    # TODO: Move literal strings into a central Constants file so they can be easily changed\n",
    "    number_of_answers = question_parameters[\"Answer_number\"] if \"Answer_number\" in question_parameters.keys() else 4\n",
    "    answer_type = question_parameters[\"Answer_type\"] if \"Answer_type\" in question_parameters.keys() else None\n",
    "    \n",
    "    incorrect_answers = []\n",
    "    temp_books = [i for i in Constants.ITIL_3_BOOKS if i not in correct_concept.book]\n",
    "    # print(temp_books)\n",
    "    \n",
    "    while(len(incorrect_answers) < int(number_of_answers) - 1):\n",
    "        # print(f\"ontology.keys: {ontology.keys()}\")\n",
    "        # random_int = random.randint(0, len(ontology))\n",
    "        # print(f\"random_int: {random_int}\")\n",
    "        # print(f\"len(ontology): {len(ontology)}\")\n",
    "        # \n",
    "        # incorrect_concept_name = list(ontology.keys())[random_int]\n",
    "        incorrect_concept_name = list(ontology.keys())[random.randint(0, len(ontology))]\n",
    "        \n",
    "        if incorrect_concept_name != correct_concept.name and answer_type == \"definition\":\n",
    "            incorrect_answers.append(get_definition(incorrect_concept_name))\n",
    "        elif incorrect_concept_name != correct_concept.name and answer_type == \"concept.name\":\n",
    "            incorrect_answers.append(incorrect_concept_name)\n",
    "        elif incorrect_concept_name != correct_concept.name and answer_type == \"books\":\n",
    "            if len(temp_books) > 0:\n",
    "                book = temp_books[random.randint(0, len(temp_books) - 1)]\n",
    "                temp_books.remove(book)\n",
    "                incorrect_answers.append(book)\n",
    "            else:\n",
    "                break\n",
    "            # print(\"-\" * 100)\n",
    "            # print(correct_concept.book)\n",
    "            # print(Constants.ITIL_3_BOOKS)\n",
    "            # print(temp_books)\n",
    "            # temp_books = [i for i in correct_concept.book]\n",
    "            # incorrect_answers.append(ITIL_3_BOOKS[])\n",
    "    \n",
    "    # TODO: Make answer picker more generalised\n",
    "    \n",
    "    return incorrect_answers\n",
    "    \n",
    "def get_random_attribute_without_concept_name(concept):\n",
    "    random_attribute = concept.attributes[random.randint(0, len(concept.attributes) - 1)].replace(concept.name, \"_\" * 10)\n",
    "    # print(random_attribute)\n",
    "    \n",
    "    if \"definition::\" in random_attribute:\n",
    "        random_attribute = f\"The definition of {'_' * 10} is: {random_attribute.replace('definition::', '')}\"\n",
    "        \n",
    "    # return concept.attributes[].replace(\"{!concept.name}\", \"_\" * 10)\n",
    "    return random_attribute\n",
    "    \n",
    "def replace_template_variables(template_format_string, concept):\n",
    "    replaced_string = template_format_string\n",
    "    \n",
    "    # print(replaced_string)\n",
    "    \n",
    "    # get_random_attribute_without_concept_name(concept)\n",
    "    \n",
    "    if \"{!concept.name}\" in replaced_string:\n",
    "        replaced_string_replacer = get_random_attribute_without_concept_name(concept)\n",
    "        replaced_string = replaced_string.replace(\"{!concept.name}\", replaced_string_replacer)\n",
    "    \n",
    "    # replaced_string = replaced_string.replace(\"{!concept.name}\", \"_\" * 10)\n",
    "    replaced_string = replaced_string.replace(\"{newline}\", \"\\n\")\n",
    "    replaced_string = replaced_string.replace(\"{concept.name}\", concept.name)\n",
    "    replaced_string = replaced_string.replace(\"{concept.definition}\", concept.definition)\n",
    "    if len(concept.book) > 0:\n",
    "        replaced_string = replaced_string.replace(\"{concept.book}\", concept.book[random.randint(0, len(concept.book) - 1)])\n",
    "    else:\n",
    "        replaced_string = replaced_string.replace(\"{concept.book}\", \"Not in a specific book\")\n",
    "    \n",
    "    # replaced_string = re.sub(\"{.*?\\!concept\\.name.*?}\", \"__________\", replaced_string)\n",
    "    # replaced_string = re.sub(\"{.*?newline}.*?\", \"\\n\", replaced_string)\n",
    "    # replaced_string = re.sub(\"{.*?concept\\.name.*?}\", concept.name, replaced_string)\n",
    "    # replaced_string = re.sub(\"{.*?concept\\.definition.*?}\", concept.definition, replaced_string)\\\n",
    "    # replaced_string = re.sub(\"{.*?concept\\.relation\\.attribute*?}\", concept.attribute, replaced_string)\n",
    "    # replaced_string = re.sub(\"{.*?concept\\.book.*?}\", concept.book, replaced_string)\n",
    "    \n",
    "    # print(replaced_string)\n",
    "    \n",
    "    return replaced_string\n",
    "\n",
    "def parse_template_question(question_template_to_parse, concept_name=\"\", debug=False):\n",
    "    template_list = question_template_to_parse.split(\"::\")\n",
    "    question_parameters = template_list[0].split(\"(\")[1].replace(\")\", \"\").split(\",\")    \n",
    "    question_parameters = {item.split(\"=\")[0]: item.split(\"=\")[1] for item in question_parameters}\n",
    "    question = template_list[1].split(\"->\")[1]\n",
    "    answer = template_list[2].split(\"->\")[1]\n",
    "            \n",
    "    if concept_name == \"\":\n",
    "        concept_name = list(ontology.keys())[random.randint(0, len(ontology))]     \n",
    "                \n",
    "    main_concept = Concept(concept_name, ontology)\n",
    "    \n",
    "    if debug is True:\n",
    "            print(f\"\"\"\n",
    "{\"-\" * 100}\n",
    "Concept:\n",
    "    Name = {main_concept.name}\n",
    "    Definition = {main_concept.definition}\n",
    "    Books = {main_concept.book}\n",
    "    Attributes = {main_concept.attributes}\n",
    "{\"-\" * 100}\n",
    "            \"\"\")\n",
    "        \n",
    "    question = replace_template_variables(question, main_concept)\n",
    "    answer = replace_template_variables(answer, main_concept)\n",
    "    \n",
    "    answer_list = [answer]\n",
    "                \n",
    "    answer_list = answer_list + get_incorrect_answers(main_concept, question_parameters)\n",
    "            \n",
    "    return Question(question, answer_list, question_parameters)\n",
    "\n",
    "def parse_template(template_to_parse, concept_name=\"\", debug=False):\n",
    "    if \"question_template\" in template_to_parse.split(\"::\")[0]:\n",
    "        return parse_template_question(template_to_parse, concept_name=concept_name, debug=debug)\n",
    "    else:\n",
    "        print(f\"Error parsing template {template_to_parse}\")\n",
    "        return None\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Complete the following:\n The definition of __________ is:  The third level in a hierarchy of Support Groups involved in the resolution of Incidents and investigation of Problems. Each level contains more specialist skills, or has more time or other resources.\n- 1: Attribute\n- 2: Third-line Support\n- 3: Planned Downtime\n- 4: Transaction\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "parsed_template = parse_template(question_template)\n",
    " \n",
    "# parsed_template = parse_template(question_template)\n",
    "\n",
    "parsed_template.format_output(output_type=\"print\")\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     print(nltk.word_tokenize(sentence))\n",
    "    \n",
    "# print(nltk.tokenize.TextTilingTokenizer().tokenize(glossary_definitions))\n",
    "# print(nltk.word_tokenize(glossary_definitions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# for concept in ontology:\n",
    "#     print(concept)\n",
    "#     for relationship in ontology[concept]:\n",
    "#         print(f\"\\t-{relationship}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# # Code from https://spacy.io/\n",
    "# import spacy\n",
    "# \n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# \n",
    "# text = \"Event Management is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\"\n",
    "# \n",
    "# doc = nlp(text)\n",
    "# \n",
    "# # # Analyze syntax\n",
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "# print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "# \n",
    "# print(\"-\" * 100)\n",
    "# \n",
    "# # Find named entities, phrases and concepts\n",
    "# for entity in doc.ents:\n",
    "#     print(entity.text, entity.label_)\n",
    "# \n",
    "# print(\"-\" * 100)\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_)\n",
    "# \n",
    "# print(\"-\" * 100)\n",
    "# for token in doc:\n",
    "#     print(token.text, token.dep_)\n",
    "\n",
    "# from spacy import displacy\n",
    "# displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 140})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "What is the definition for \"Event Management\"?\n- 1:  The Process responsible for managing Events throughout their Lifecycle. Event Management is one of the main Activities of IT Operations.\n- 2: (Continual Service Improvement) An understanding of the Service Provider and IT Services from the point of view of the Business, and an understanding of the Business from the point of view of the Service Provider.\n- 3: Something that influences Strategy, Objectives or Requirements. For example, new legislation or the actions of competitors.\n- 4: (Service Design) Used to refer to Resources that are not required to deliver the Live IT Services, but are available to support IT Service Continuity Plans. For example a Standby data centre may be maintained to support Hot Standby, Warm Standby or Cold Standby arrangements.\n\nWhich book is the concept \"Event Management\" in?\n- 1: Service Operation\n- 2: Not in a specific book\n- 3: Service Management\n- 4: Service Transition\n\nComplete the following:\n While it is true that monitoring is required to detect and track these notifications , monitoring is broader than __________ .\n- 1: Capacity Planning\n- 2: Work in Progress (WIP)\n- 3: Event Management\n- 4: Version\n\nComplete the following:\n 4.1.8 Metrics For each measurement period in question , the metrics to check on the effectiveness and efficiency of the __________ process should include the following : ï¿½ Number of events by category ï¿½ Number of events by significance ï¿½ Number and percentage of events that required human intervention and whether this was performed ï¿½ Number and percentage of events that resulted in incidents or changes ï¿½ Number and percentage of events caused by existing problems or Known Errors .\n- 1: Event Management\n- 2: Continual Service Improvement (CSI)\n- 3: Workaround\n- 4: Monitor Control Loop\n\nComplete the following:\n 4.1.10.4 Identification of thresholds Thresholds themselves are not set and managed through __________ .\n- 1: Event Management\n- 2: Outsourcing\n- 3: ISO/IEC 27001\n- 4: Problem Management\n\nComplete the following:\n No matter how thoroughly __________ is prepared , there will be classes of events that are not properly filtered .\n- 1: Event Management\n- 2: Early Life Support\n- 3: Information Security Policy\n- 4: Definitive Media Library (DML)\n\nComplete the following:\n Nor does it mean that , once designed and agreed , __________ becomes static ï¿½ day-to-day operations will define additional events , priorities , alerts and other improvements that will feed through the Continual Improvement process back into Service Strategy , Service Design etc .\n- 1: Policy\n- 2: Call\n- 3: Event Management\n- 4: IT Operations Control\n\nComplete the following:\n ï¿½ __________ is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions .\n- 1: User Profile (UP)\n- 2: Request Fulfilment\n- 3: Event Management\n- 4: Technical Support\n\nComplete the following:\n Thorough design of the event detection and alert mechanisms requires the following : ï¿½ Business knowledge in relationship to any business processes being managed via __________ ï¿½ Detailed knowledge of the Service Level Requirements of the service being supported by each CI ï¿½ Knowledge of who is going to be supporting the CI ï¿½ Knowledge of what constitutes normal and abnormal operation of the CI ï¿½ Knowledge of the significance of multiple similar events ( on the same CI or various similar CIs ï¿½ An understanding of what they need to know to support the CI effectively ï¿½ Information that can help in the diagnosis of problems with the CI ï¿½ Familiarity with incident prioritization and categorization codes so that if it is necessary to create an Incident Record , these codes can be provided ï¿½ Knowledge of other CIs that may be dependent on the affected CI , or those CIs on which it depends ï¿½ Availability of Known Error information from vendors or from previous experience .\n- 1: Application Sizing\n- 2: Unit Cost\n- 3: Event Management\n- 4: IT Infrastructure\n\nComplete the following:\n ï¿½ Design new services with __________ in mind ( this is discussed in detail in paragraph 4.1.10 ) .\n- 1: Off-shore\n- 2: Event Management\n- 3: Business Impact Analysis (BIA)\n- 4: IT Service Continuity Management (ITSCM)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# for concept in ontology:\n",
    "#     print(concept)attribute.replace\n",
    "\n",
    "# print(ontology[\"service\"][1])\n",
    "\n",
    "\n",
    "def generate_parsed_templates(number_of_questions=1, debug=False):\n",
    "    parsed_templates = []\n",
    "    \n",
    "    for i in range(0, number_of_questions):\n",
    "        # question_template = templates[0] if i is 0 else templates[random.randint(1, len(templates) - 1)]\n",
    "        if i == 0:\n",
    "            question_template = templates[0]\n",
    "        elif i == 1:\n",
    "            question_template = templates[2]\n",
    "        else:\n",
    "            question_template = templates[1]\n",
    "        # question_template = templates[2]\n",
    "        parsed_templates.append(parse_template(question_template, concept_name=\"Event Management\", debug=debug if i is 0 else False))\n",
    "    return parsed_templates\n",
    "        \n",
    "        \n",
    "                \n",
    "    \n",
    "parsed_templates = generate_parsed_templates(number_of_questions=10)\n",
    "for parsed_template in parsed_templates:\n",
    "    # print(parsed_template.correct_answer)\n",
    "    parsed_template.format_output()\n",
    "\n",
    "# parsed_template = parse_template(question_template, concept_name=\"Event Management\", debug=True)\n",
    "# parsed_template.format_output()\n",
    "\n",
    "# for concept in ontology:\n",
    "#     # concept = ontology[\"service\"]\n",
    "#     for index, sentence in enumerate(ontology[concept]):\n",
    "#         # print(sentence)\n",
    "#         # print(index)\n",
    "#         # print(concept)\n",
    "#         if \"definition::\" in sentence:\n",
    "#             continue\n",
    "#             \n",
    "#         # print(sentence.replace(\"service\", \"{concept}\"))\n",
    "#         ontology[concept][index] = sentence.replace(concept, \"{!concept.name}\")\n",
    "#                 \n",
    "#         # print(sentence.concept.keys()[0])\n",
    "        \n",
    "# concept_name = \"service\"\n",
    "# print(get_random_attribute_without_concept_name(Concept(concept_name, ontology)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unittests\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "test_Concept (__main__.TestUnitTests) ... ",
      "ok\ntest_Question (__main__.TestUnitTests) ... ",
      "ok\ntest_generate_parsed_templates (__main__.TestUnitTests) ... ",
      "ok\n\n----------------------------------------------------------------------\nRan 3 tests in 0.003s\n\nOK\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x216e8b20a20>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestUnitTests(unittest.TestCase):\n",
    "\n",
    "    def test_generate_parsed_templates(self):\n",
    "        parsed_templates = generate_parsed_templates(1, debug=False)[0]\n",
    "        self.assertIsInstance(parsed_templates, Question)\n",
    "        \n",
    "    ### Testing classes ###\n",
    "    # Question\n",
    "    def test_Question(self):\n",
    "        question = Question(\n",
    "            \"Question example unit test 1\",\n",
    "            [\"Answer 1\", \"Answer 2\", \"Answer 3\", \"Answer 4\"],\n",
    "            {'Question_number': '1', 'Answer_number': '4', 'Question_type': 'attribute.replace', 'Answer_type': 'concept.name'})\n",
    "        self.assertEqual(question.question_string, \"Question example unit test 1\")\n",
    "        self.assertEqual(question.answer_list, [\"Answer 1\", \"Answer 2\", \"Answer 3\", \"Answer 4\"])\n",
    "        self.assertEqual(question.parameters, {'Question_number': '1', 'Answer_number': '4', 'Question_type': 'attribute.replace', 'Answer_type': 'concept.name'})\n",
    "        self.assertEqual(question.correct_answer, \"Answer 1\")\n",
    "        \n",
    "    # Concept\n",
    "    def test_Concept(self):\n",
    "        ontology = {'Acceptance': ['definition::Formal agreement that an IT Service, Process, Plan, or other Deliverable is complete, accurate, Reliable and meets its specified Requirements. Acceptance is usually preceded by Evaluation or Testing and is often required before proceeding to the next stage of a Project or Process.', 'Other Problem Records , and corresponding Known Error Records , may be triggered in testing , particularly the latter stages of testing such as User Acceptance Testing/Trials ( UAT ) , if a decision is made to go ahead with a release even though some faults are known .'], 'Access Management': ['definition::(Service Operation) The Process responsible for allowing Users to make use of IT Services, data, or other Assets.', 'ï¿½ Access Management : this is the process of granting authorized users the right to use a service , while restricting access to non-authorized users .']}\n",
    "        \n",
    "        concept_1 = Concept(\n",
    "            \"Acceptance\",\n",
    "            ontology\n",
    "        )\n",
    "        \n",
    "        self.assertEqual(concept_1.name, \"Acceptance\")\n",
    "        self.assertEqual(concept_1.ontology, ontology)\n",
    "        self.assertFalse(concept_1.book)\n",
    "        \n",
    "        concept_2 = Concept(\n",
    "            \"Access Management\",\n",
    "            ontology\n",
    "        )\n",
    "        \n",
    "        self.assertEqual(concept_2.name, \"Access Management\")\n",
    "        self.assertEqual(concept_2.ontology, ontology)\n",
    "        self.assertIsInstance(concept_2.book, list)\n",
    "        self.assertEqual(concept_2.book, [\"Service Operation\"])\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}