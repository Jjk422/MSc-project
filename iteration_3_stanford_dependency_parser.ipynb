{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".",
      "\n",
      "Would you like to download the models for: en_ewt now? (Y/n)",
      "\n",
      "\n",
      "Default download directory: C:\\Users\\User\\stanfordnlp_resources",
      "\n",
      "Hit enter to continue or type an alternate directory.",
      "\n",
      "\n",
      "Downloading models for: en_ewt",
      "\n",
      "Download location: C:\\Users\\User\\stanfordnlp_resources\\en_ewt_models.zip",
      "\n",
      "\n",
      "Download complete.  Models saved to: C:\\Users\\User\\stanfordnlp_resources\\en_ewt_models.zip",
      "\n",
      "Extracting models file for: en_ewt",
      "\n",
      "Cleaning up...",
      "Done.",
      "\n",
      "Use device: cpu",
      "\n",
      "---",
      "\n",
      "Loading: tokenize",
      "\n",
      "With settings: ",
      "\n",
      "{'model_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}",
      "\n",
      "---",
      "\n",
      "Loading: lemma",
      "\n",
      "With settings: ",
      "\n",
      "{'model_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}",
      "\n",
      "Building an attentional Seq2Seq model...",
      "\n",
      "Using a Bi-LSTM encoder",
      "\n",
      "Using soft attention for LSTM.",
      "\n",
      "Finetune all embeddings.",
      "\n",
      "[Running seq2seq lemmatizer with edit classifier]",
      "\n",
      "---",
      "\n",
      "Loading: pos",
      "\n",
      "With settings: ",
      "\n",
      "{'model_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\User\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}",
      "\n",
      "Done loading processors!",
      "\n",
      "---",
      "\n",
      "<Token index=1;words=[<Word index=1;text=Event;lemma=event;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=2;words=[<Word index=2;text=Management;lemma=management;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=3;words=[<Word index=3;text=is;lemma=be;upos=AUX;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>",
      "\n",
      "<Token index=4;words=[<Word index=4;text=the;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>",
      "\n",
      "<Token index=5;words=[<Word index=5;text=process;lemma=process;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=6;words=[<Word index=6;text=that;lemma=that;upos=PRON;xpos=WDT;feats=PronType=Rel>]>",
      "\n",
      "<Token index=7;words=[<Word index=7;text=monitors;lemma=monitor;upos=VERB;xpos=VBZ;feats=Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin>]>",
      "\n",
      "<Token index=8;words=[<Word index=8;text=all;lemma=all;upos=DET;xpos=DT;feats=_>]>",
      "\n",
      "<Token index=9;words=[<Word index=9;text=events;lemma=event;upos=NOUN;xpos=NNS;feats=Number=Plur>]>",
      "\n",
      "<Token index=10;words=[<Word index=10;text=that;lemma=that;upos=PRON;xpos=WDT;feats=PronType=Rel>]>",
      "\n",
      "<Token index=11;words=[<Word index=11;text=occur;lemma=occur;upos=VERB;xpos=VBP;feats=Mood=Ind|Tense=Pres|VerbForm=Fin>]>",
      "\n",
      "<Token index=12;words=[<Word index=12;text=through;lemma=through;upos=ADP;xpos=IN;feats=_>]>",
      "\n",
      "<Token index=13;words=[<Word index=13;text=the;lemma=the;upos=DET;xpos=DT;feats=Definite=Def|PronType=Art>]>",
      "\n",
      "<Token index=14;words=[<Word index=14;text=IT;lemma=it;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=15;words=[<Word index=15;text=infrastructure;lemma=infrastructure;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=16;words=[<Word index=16;text=to;lemma=to;upos=PART;xpos=TO;feats=_>]>",
      "\n",
      "<Token index=17;words=[<Word index=17;text=allow;lemma=allow;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>",
      "\n",
      "<Token index=18;words=[<Word index=18;text=for;lemma=for;upos=ADP;xpos=IN;feats=_>]>",
      "\n",
      "<Token index=19;words=[<Word index=19;text=normal;lemma=normal;upos=ADJ;xpos=JJ;feats=Degree=Pos>]>",
      "\n",
      "<Token index=20;words=[<Word index=20;text=operation;lemma=operation;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=21;words=[<Word index=21;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_>]>",
      "\n",
      "<Token index=22;words=[<Word index=22;text=also;lemma=also;upos=ADV;xpos=RB;feats=_>]>",
      "\n",
      "<Token index=23;words=[<Word index=23;text=to;lemma=to;upos=PART;xpos=TO;feats=_>]>",
      "\n",
      "<Token index=24;words=[<Word index=24;text=detect;lemma=detect;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>",
      "\n",
      "<Token index=25;words=[<Word index=25;text=and;lemma=and;upos=CCONJ;xpos=CC;feats=_>]>",
      "\n",
      "<Token index=26;words=[<Word index=26;text=escalate;lemma=escalate;upos=VERB;xpos=VB;feats=VerbForm=Inf>]>",
      "\n",
      "<Token index=27;words=[<Word index=27;text=exception;lemma=exception;upos=NOUN;xpos=NN;feats=Number=Sing>]>",
      "\n",
      "<Token index=28;words=[<Word index=28;text=conditions;lemma=condition;upos=NOUN;xpos=NNS;feats=Number=Plur>]>",
      "\n",
      "<Token index=29;words=[<Word index=29;text=.;lemma=.;upos=PUNCT;xpos=.;feats=_>]>",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "\r  0%|          | 0.00/235M [00:00<?, ?B/s]",
      "\r 29%|██▊       | 67.1M/235M [00:31<01:17, 2.15MB/s]",
      "\r 29%|██▊       | 67.1M/235M [00:50<01:17, 2.15MB/s]",
      "\r 57%|█████▋    | 134M/235M [01:49<01:07, 1.48MB/s] ",
      "\r 57%|█████▋    | 134M/235M [02:00<01:07, 1.48MB/s]",
      "\r 86%|████████▌ | 201M/235M [02:54<00:25, 1.31MB/s]",
      "\r 86%|████████▌ | 201M/235M [03:10<00:25, 1.31MB/s]",
      "\r100%|██████████| 235M/235M [04:02<00:00, 873kB/s] ",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('en')\n",
    "nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos\")\n",
    "doc = nlp(\"Event Management is the process that monitors all events that occur through the IT infrastructure to allow for normal operation and also to detect and escalate exception conditions.\")\n",
    "doc.sentences[0].print_tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "text: Event         \tlemma: event\tupos: NOUN\txpos: NN",
      "\n",
      "text: Management         \tlemma: management\tupos: NOUN\txpos: NN",
      "\n",
      "text: is         \tlemma: be\tupos: AUX\txpos: VBZ",
      "\n",
      "text: the         \tlemma: the\tupos: DET\txpos: DT",
      "\n",
      "text: process         \tlemma: process\tupos: NOUN\txpos: NN",
      "\n",
      "text: that         \tlemma: that\tupos: PRON\txpos: WDT",
      "\n",
      "text: monitors         \tlemma: monitor\tupos: VERB\txpos: VBZ",
      "\n",
      "text: all         \tlemma: all\tupos: DET\txpos: DT",
      "\n",
      "text: events         \tlemma: event\tupos: NOUN\txpos: NNS",
      "\n",
      "text: that         \tlemma: that\tupos: PRON\txpos: WDT",
      "\n",
      "text: occur         \tlemma: occur\tupos: VERB\txpos: VBP",
      "\n",
      "text: through         \tlemma: through\tupos: ADP\txpos: IN",
      "\n",
      "text: the         \tlemma: the\tupos: DET\txpos: DT",
      "\n",
      "text: IT         \tlemma: it\tupos: NOUN\txpos: NN",
      "\n",
      "text: infrastructure         \tlemma: infrastructure\tupos: NOUN\txpos: NN",
      "\n",
      "text: to         \tlemma: to\tupos: PART\txpos: TO",
      "\n",
      "text: allow         \tlemma: allow\tupos: VERB\txpos: VB",
      "\n",
      "text: for         \tlemma: for\tupos: ADP\txpos: IN",
      "\n",
      "text: normal         \tlemma: normal\tupos: ADJ\txpos: JJ",
      "\n",
      "text: operation         \tlemma: operation\tupos: NOUN\txpos: NN",
      "\n",
      "text: and         \tlemma: and\tupos: CCONJ\txpos: CC",
      "\n",
      "text: also         \tlemma: also\tupos: ADV\txpos: RB",
      "\n",
      "text: to         \tlemma: to\tupos: PART\txpos: TO",
      "\n",
      "text: detect         \tlemma: detect\tupos: VERB\txpos: VB",
      "\n",
      "text: and         \tlemma: and\tupos: CCONJ\txpos: CC",
      "\n",
      "text: escalate         \tlemma: escalate\tupos: VERB\txpos: VB",
      "\n",
      "text: exception         \tlemma: exception\tupos: NOUN\txpos: NN",
      "\n",
      "text: conditions         \tlemma: condition\tupos: NOUN\txpos: NNS",
      "\n",
      "text: .         \tlemma: .\tupos: PUNCT\txpos: .",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "with open(\"ITIL Books/ITIL 3/Service operation glossary/definitions.txt\") as file:\n",
    "    glossary_definitions = file.read()\n",
    "\n",
    "definitions = [i.lower().replace(\"\\n\", \" \").split('::') for i in glossary_definitions.split('\\n\\n')]\n",
    "\n",
    "concepts_from_definitions = [i[0] for i in definitions]\n",
    "# print(concepts_from_definitions)\n",
    "\n",
    "with open(\"ITIL Books/ITIL 3/Service operation chapter 4/Service operation chapter 4 - 4.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# print(text)\n",
    "\n",
    "ontology = {}\n",
    "\n",
    "for index in range(0, len(definitions)):\n",
    "    ontology[definitions[index][0]] = [f\"definition:: {definitions[index][1]}\"]\n",
    "\n",
    "# print(ontology)\n",
    "\n",
    "import nltk\n",
    "# Tokenize into sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Split sentences into words\n",
    "sentences = [nltk.word_tokenize(i.lower().replace(\"\\n\", \" \")) for i in sentences]\n",
    "\n",
    "sentences_containing_concepts = {}\n",
    "for sentence in sentences:\n",
    "    # print(\"---\")\n",
    "    count = 0\n",
    "    for concept in concepts_from_definitions:\n",
    "        concept_list = concept.split()\n",
    "        \n",
    "        # print(concept_list)\n",
    "        \n",
    "        # count = count + 1\n",
    "        # if count is 1:\n",
    "        #     print(sentence)\n",
    "        if concept_list[0] in sentence:\n",
    "            add_to_sentences_containing_concepts = True\n",
    "            \n",
    "            if len(concept_list) > 1:\n",
    "                current_word_index = sentence.index(concept_list[0])\n",
    "                concept_word_length_count = 1\n",
    "                for concept_word in concept_list[1::]:\n",
    "                    current_word_index = current_word_index + 1\n",
    "                    \n",
    "                    if sentence[current_word_index] != concept_word:\n",
    "                        add_to_sentences_containing_concepts = False\n",
    "                        break\n",
    "                                            \n",
    "                    # print(concept)\n",
    "                    # print(concept_word)            \n",
    "                \n",
    "            if add_to_sentences_containing_concepts is True:\n",
    "                sentence_string = \" \".join(sentence)\n",
    "                if sentence_string not in sentences_containing_concepts.keys():\n",
    "                    sentences_containing_concepts[sentence_string] = []\n",
    "                sentences_containing_concepts[sentence_string].append(concept)\n",
    "            \n",
    "\n",
    "# sentences_containing_concepts = [i for i in sentences if i in concepts_from_definitions]\n",
    " \n",
    "# for sentence in sentences_containing_concepts:\n",
    "#     print(sentence)\n",
    "\n",
    "# print(sentences_containing_concepts)\n",
    "\n",
    "# print(ontology)\n",
    "\n",
    "for sentence, concepts in sentences_containing_concepts.items():\n",
    "    for concept in concepts:\n",
    "        ontology[concept].append(sentence)\n",
    "\n",
    "# TODO: In definitions replace (book name) with own in book relation\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "What is the definition for release record ?",
      "\n",
      "-  (service transition) a record in the cmdb that defines the content of a release. a release record has relationships with all configuration items that are affected by the release.",
      "\n",
      "-  cost resulting from running the it services. often repeating payments. for example staff costs, hardware maintenance and electricity (also known as â€˜current expenditureâ€™ or â€˜revenue expenditureâ€™). see also capital expenditure.",
      "\n",
      "-  the implementation and management of quality it services that meet the needs of the business. it service management are performed by it service providers through an appropriate mix of people, process and information technology. see also service management.",
      "\n",
      "-  (service operation) the function responsible for monitoring and control of the it services and it infrastructure. see also operations bridge.",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "question_template = \"question_template(Question_number=1,Answer_number=4)::Question->What is the definition for {concept.name}?::Answer->{concept.definition}\"\n",
    "\n",
    "# print(question_template)\n",
    "\n",
    "def get_definition(concept_name):\n",
    "    return [i.split(\"::\")[1] for i in ontology[concept_name] if i.split(\"::\")[0] == \"definition\"][0]\n",
    "\n",
    "class Concept:\n",
    "    def __init__(self, name, ontology):\n",
    "        self.name = name\n",
    "        self.ontology = ontology\n",
    "        self.attributes = ontology[self.name]\n",
    "        # self.definition = [i.split(\"::\")[1] for i in ontology[self.name] if i.split(\"::\")[0] == \"definition\"][0]\n",
    "        self.definition = get_definition(self.name)\n",
    "        # self.book = re.findall(\"^\\w*\\(.*?\\)\", self.definition)\n",
    "        # self.definition = re.sub(\"^\\w*\\(.*?\\)\", \"\", self.definition)         \n",
    "        \n",
    "    # def name(self):\n",
    "    #     return self.name\n",
    "    # \n",
    "    # def definition(self):\n",
    "    #     return self.definition\n",
    "    # \n",
    "    # def attributes(self):\n",
    "    #     return self.attributes\n",
    "    \n",
    "\n",
    "class Question:\n",
    "    def __init__(self, question_string, answer_list, question_params):\n",
    "        self.question_string = question_string\n",
    "        self.answer_list = answer_list\n",
    "        self.parameters = question_params\n",
    "        \n",
    "    def print_question_and_answer(self):\n",
    "        print(self.question_string)\n",
    "        for answer in self.answer_list:\n",
    "            print(f\"- {answer}\")\n",
    "        \n",
    "    def format_output(self):\n",
    "        self.print_question_and_answer()\n",
    "        \n",
    "def get_incorrect_answers(correct_concept, number_of_incorrect_answers):\n",
    "    incorrect_answers = []\n",
    "    \n",
    "    while(len(incorrect_answers) < int(number_of_incorrect_answers) - 1):\n",
    "        incorrect_concept_name = list(ontology.keys())[random.randint(0, len(ontology))]\n",
    "        if incorrect_concept_name != correct_concept.name:\n",
    "            incorrect_answers.append(get_definition(incorrect_concept_name))\n",
    "    \n",
    "    return incorrect_answers\n",
    "    \n",
    "def replace_template_variables(template_format_string, concept):\n",
    "    replaced_string = template_format_string\n",
    "\n",
    "    replaced_string = re.sub(\"{.*?concept\\.name.*?}\", concept.name, template_format_string)\n",
    "    replaced_string = re.sub(\"{.*?concept\\.definition.*?}\", concept.definition, replaced_string)\\\n",
    "    # replaced_string = re.sub(\"{.*?concept\\.relation\\.attribute*?}\", concept.attrubute, replaced_string)\n",
    "    \n",
    "    return replaced_string\n",
    "\n",
    "def parse_template_question(question_template_to_parse, concept_name=\"\"):\n",
    "    template_list = question_template_to_parse.split(\"::\")\n",
    "    question_parameters = template_list[0].split(\"(\")[1].replace(\")\", \"\").split(\",\")    \n",
    "    question_parameters = {item.split(\"=\")[0]: item.split(\"=\")[1] for item in question_parameters}\n",
    "    question = template_list[1].split(\"->\")[1]\n",
    "    answer = template_list[2].split(\"->\")[1]\n",
    "        \n",
    "    if concept_name == \"\":\n",
    "        concept_name = list(ontology.keys())[random.randint(0, len(ontology))]\n",
    "        \n",
    "    # print(concept_name)\n",
    "    \n",
    "    concept = Concept(concept_name, ontology)\n",
    "    \n",
    "    # print(concept.definition)\n",
    "    # print(concept.book)\n",
    "    \n",
    "    # print(concept.name)\n",
    "    # print(concept.definition)\n",
    "    # print(concept.attributes)\n",
    "    \n",
    "    # question = re.sub(\"{.*?concept\\.name.*?}\", concept.name, question)\n",
    "    # question = re.sub(\"{.*?concept\\.definition.*?}\", concept.definition, question)\n",
    "    # # question = re.sub(\"{.*?concept\\.relation\\.attribute*?}\", concept.attrubute, question)\n",
    "    # \n",
    "    # answer = re.sub(\"{.*?concept\\.name.*?}\", concept_name, answer)\n",
    "    # answer = re.sub(\"{.*?concept\\.definition.*?}\", concept.definition, answer)\n",
    "    # # answer = re.sub(\"{.*?concept\\.relation\\.definition*?}\", ontology[concept_name], answer)\n",
    "    \n",
    "    # print(question)\n",
    "    # print(answer)\n",
    "    \n",
    "    question = replace_template_variables(question, concept)\n",
    "    answer = replace_template_variables(answer, concept)\n",
    "    \n",
    "    answer_list = [answer]\n",
    "            \n",
    "    if \"Answer_number\" in question_parameters.keys():\n",
    "        answer_list = answer_list + get_incorrect_answers(concept, question_parameters[\"Answer_number\"])\n",
    "    else:\n",
    "        answer_list = answer_list + get_incorrect_answers(concept, 4)\n",
    "                \n",
    "    return Question(question, answer_list, question_parameters)\n",
    "\n",
    "def parse_template(template_to_parse, concept_name=\"\"):\n",
    "    if \"question_template\" in template_to_parse.split(\"::\")[0]:\n",
    "        return parse_template_question(template_to_parse)\n",
    "    else:\n",
    "        print(f\"Error parsing template {template_to_parse}\")\n",
    "        return None\n",
    " \n",
    "parsed_template = parse_template(question_template)\n",
    "\n",
    "parsed_template.format_output()\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     print(nltk.word_tokenize(sentence))\n",
    "    \n",
    "# print(nltk.tokenize.TextTilingTokenizer().tokenize(glossary_definitions))\n",
    "# print(nltk.word_tokenize(glossary_definitions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}